{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "from itertools import combinations\n",
    "# input_file = \"small1.csv\"\n",
    "input_file = \"small1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import os\n",
    "\n",
    "sc = SparkContext('local[*]', 'wordCount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"small1.csv\")\n",
    "data2 = pd.read_csv(\"small2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>18</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>18</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>19</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>19</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>19</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  business_id\n",
       "0         1          100\n",
       "1         1           98\n",
       "2         1          101\n",
       "3         1          102\n",
       "4         2          101\n",
       "..      ...          ...\n",
       "69       18           99\n",
       "70       18           97\n",
       "71       19          102\n",
       "72       19           97\n",
       "73       19           98\n",
       "\n",
       "[74 rows x 2 columns]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "0.9836781024932861\n",
      "[0, 2, 4, 3, 3, 1, 2, 2, 3, 4]\n",
      "##############################\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "s = 3\n",
    "\n",
    "\n",
    "\n",
    "def merge_basket_2d(baskets):\n",
    "    a = list(baskets)\n",
    "    collected_basket = []\n",
    "    for i in a:\n",
    "        collected_basket += [i]\n",
    "    return collected_basket\n",
    "\n",
    "\n",
    "def count_singleton(basket_list_2d):\n",
    "    count_num_dict = {}\n",
    "    for basket in basket_list_2d:\n",
    "        for item in basket:\n",
    "            count_num_dict[item] = count_num_dict.get(item, 0) + 1\n",
    "    return count_num_dict\n",
    "\n",
    "\n",
    "def scaled_down_support_threshold(basket_list_2d, support, entire_size):\n",
    "    partition_size = len(basket_list_2d)\n",
    "    return math.ceil((partition_size / entire_size) * int(support))\n",
    "\n",
    "\n",
    "def get_candidate(count_dict, scaled_down_support_threshold):\n",
    "    candidate_list = []\n",
    "    for i in count_dict.keys():\n",
    "        if count_dict[i] >= scaled_down_support_threshold:\n",
    "            candidate_list.append(i)\n",
    "\n",
    "    return candidate_list\n",
    "\n",
    "\n",
    "def get_singleton_candidate_from_paritition(basket_list_2d, ps):\n",
    "    return get_candidate(count_singleton(basket_list_2d), ps)\n",
    "\n",
    "\n",
    "def count_pairs(basket_list_2d, singleton_candidate):\n",
    "    count_dict = {}\n",
    "    for basket in basket_list_2d:\n",
    "        for b1 in basket:\n",
    "            for b2 in basket:\n",
    "                if b1 < b2:\n",
    "                    if b1 in singleton_candidate:\n",
    "                        if b2 in singleton_candidate:\n",
    "                            count_dict[(b1, b2)] = count_dict.get((b1, b2), 0) + 1\n",
    "\n",
    "    return count_dict\n",
    "\n",
    "\n",
    "def get_pair_candidate(basket_list_2d, singleton_candidate, ps):\n",
    "    pair_candi_list = []\n",
    "    pair_count = count_pairs(basket_list_2d, singleton_candidate)\n",
    "    for candidate in pair_count.keys():\n",
    "        if pair_count[candidate] > ps:\n",
    "            pair_candi_list.append(candidate)\n",
    "    return pair_candi_list\n",
    "\n",
    "\n",
    "def generate_permutations(candidate_list, pair_size):\n",
    "    permu_list = []\n",
    "    if len(candidate_list) > 0:\n",
    "        for idx, itemset in enumerate(candidate_list[:-1]):\n",
    "            # print(\"idx, itemset : \", idx, itemset)\n",
    "            for itemset2 in candidate_list[idx + 1:]:\n",
    "                # print(\"itemset2 : \", itemset2)\n",
    "                if list(itemset[:-1]) == list(itemset2[:-1]):\n",
    "                    # print(itemset2[-1])\n",
    "                    # print(sorted(list(itemset) + [itemset2[-1]]))\n",
    "                    candi = sorted(list(itemset) + [itemset2[-1]])\n",
    "\n",
    "                    if candi not in permu_list:\n",
    "                        check_list = [tuple(sorted(i)) for i in list(combinations(candi, pair_size - 1))]\n",
    "                        if set(check_list).issubset(candidate_list):\n",
    "                            permu_list.append(tuple(candi))\n",
    "\n",
    "    return permu_list\n",
    "\n",
    "\n",
    "def apriori(partition, support, entire_size):\n",
    "    total_freq_itemsets = []\n",
    "\n",
    "    basket_list_2d = merge_basket_2d(partition)\n",
    "    #     max_len = max([len(basket) for basket in basket_list_2d])\n",
    "    ps = scaled_down_support_threshold(basket_list_2d, support, entire_size)\n",
    "\n",
    "    ## get singleton candidate\n",
    "    singleton_candidate = sorted(get_singleton_candidate_from_paritition(basket_list_2d, ps))\n",
    "    total_freq_itemsets += singleton_candidate\n",
    "    ## get pair candidate\n",
    "    pair_candidate = sorted(get_pair_candidate(basket_list_2d, singleton_candidate, ps))\n",
    "    total_freq_itemsets += pair_candidate\n",
    "\n",
    "    ## get candidate until no element left\n",
    "    pair_size = 2\n",
    "    candidate_list = pair_candidate\n",
    "    while len(candidate_list) > 0:\n",
    "        pair_size += 1  ## pair_size = 3\n",
    "        count_dict = {}\n",
    "        pairs_to_be_counted = generate_permutations(candidate_list, pair_size)\n",
    "        for pair in pairs_to_be_counted:\n",
    "            for bucket in basket_list_2d:\n",
    "                if set(pair).issubset(bucket):\n",
    "                    count_dict[pair] = count_dict.get(pair, 0) + 1\n",
    "        candidate_list = []\n",
    "        for candi in count_dict.keys():\n",
    "            if count_dict[candi] >= ps:\n",
    "                candidate_list.append(candi)\n",
    "\n",
    "        total_freq_itemsets += candidate_list\n",
    "\n",
    "    return total_freq_itemsets\n",
    "\n",
    "\n",
    "#     return [pair_candidate]\n",
    "\n",
    "textRDD = sc.textFile(input_file, 9)\n",
    "header = textRDD.first()  ## remove header\n",
    "tmp = textRDD.filter(lambda x: x != header)\n",
    "baskets = tmp.map(lambda x: x.split(\",\")).mapValues(lambda x: [x]).reduceByKey(lambda a, b: a + b).mapValues(\n",
    "    lambda x: sorted(list(set(x)))).map(lambda x: x[1])\n",
    "# baskets = tmp.collect()\n",
    "entire_size = tmp.count()\n",
    "\n",
    "num = tmp.mapPartitions(lambda x: [sum(1 for i in x)]).collect() ## number of items in each partition\n",
    "baskets\n",
    "\n",
    "## phase 1\n",
    "tmp = baskets.mapPartitions(lambda partition: apriori(partition, s, entire_size)).map(lambda x: (x, 1)).reduceByKey(\n",
    "    lambda a, b: a)\n",
    "candidate_itemsets = tmp.collect()\n",
    "\n",
    "num = tmp.mapPartitions(lambda x: [sum(1 for i in x)]).collect()  ## number of items in each partition\n",
    "\n",
    "for i in range(len(candidate_itemsets)):\n",
    "    if type(candidate_itemsets[i][0]) == str:\n",
    "        candidate_itemsets[i] = ((candidate_itemsets[i][0],), 1)\n",
    "\n",
    "\n",
    "# phase2\n",
    "def Map2(basket, candidate_itemsets):\n",
    "    answer_list = []\n",
    "    for candidate in candidate_itemsets:\n",
    "        if set(candidate[0]).issubset(set(basket)):\n",
    "            answer_list.append((candidate[0], 1))\n",
    "    size = len(answer_list)\n",
    "\n",
    "    return answer_list\n",
    "\n",
    "\n",
    "freq_itemsets = baskets.map(lambda x: Map2(x, candidate_itemsets)).flatMap(lambda x: x).reduceByKey(\n",
    "    lambda a, b: a + b).filter(lambda x: x[1] >= int(s)).map(lambda x: x[0]).collect()\n",
    "\n",
    "freq_itemsets = sorted(freq_itemsets, key=lambda x: (len(x), x))\n",
    "\n",
    "freq_itemsets\n",
    "#     return answer_list[0], answer_list[1], ... answer_list[-1]\n",
    "\n",
    "## end time\n",
    "time_spent = (time.time() - start_time)\n",
    "\n",
    "time_spent\n",
    "print(\"##############################\")\n",
    "print(time_spent)\n",
    "print(num)\n",
    "print(\"##############################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9836781024932861"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['97', '98'],\n",
       " ['100', '101', '102', '103', '105', '106', '107', '108', '98', '99'],\n",
       " ['100', '101', '102', '98'],\n",
       " ['97'],\n",
       " ['100', '101', '98', '99'],\n",
       " ['100', '101', '97', '99'],\n",
       " ['101', '102'],\n",
       " ['101', '97', '99'],\n",
       " ['101', '102', '103', '97', '99'],\n",
       " ['97', '98', '99'],\n",
       " ['100', '101', '102', '105', '106', '107', '108', '98'],\n",
       " ['97', '98'],\n",
       " ['101', '97', '99'],\n",
       " ['102', '97', '98'],\n",
       " ['97', '98', '99'],\n",
       " ['97', '98', '99'],\n",
       " ['102', '103', '104', '97', '98', '99'],\n",
       " ['97', '99'],\n",
       " ['102', '103', '105', '97', '98', '99']]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baskets.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('105',), 1),\n",
       " (('104',), 1),\n",
       " (('98',), 1),\n",
       " (('102',), 1),\n",
       " (('107',), 1),\n",
       " (('100', '101', '98'), 1),\n",
       " (('101',), 1),\n",
       " (('99',), 1),\n",
       " (('98', '99'), 1),\n",
       " (('106',), 1),\n",
       " (('108',), 1),\n",
       " (('101', '97'), 1),\n",
       " (('101', '102'), 1),\n",
       " (('100',), 1),\n",
       " (('100', '98'), 1),\n",
       " (('97',), 1),\n",
       " (('97', '98'), 1),\n",
       " (('101', '99'), 1),\n",
       " (('97', '99'), 1),\n",
       " (('101', '97', '99'), 1),\n",
       " (('103',), 1),\n",
       " (('100', '101'), 1),\n",
       " (('101', '98'), 1),\n",
       " (('97', '98', '99'), 1)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('100',),\n",
       " ('101',),\n",
       " ('102',),\n",
       " ('103',),\n",
       " ('105',),\n",
       " ('97',),\n",
       " ('98',),\n",
       " ('99',),\n",
       " ('100', '101'),\n",
       " ('100', '98'),\n",
       " ('101', '102'),\n",
       " ('101', '97'),\n",
       " ('101', '98'),\n",
       " ('101', '99'),\n",
       " ('97', '98'),\n",
       " ('97', '99'),\n",
       " ('98', '99'),\n",
       " ('100', '101', '98'),\n",
       " ('101', '97', '99'),\n",
       " ('97', '98', '99')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 2, 2, 0, 0, 0, 3, 1, 1, 1, 0, 0, 1, 0, 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "1.5714077949523926\n",
      "[0, 0, 0, 2, 0, 2, 4, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 3, 1, 0, 1]\n",
      "##############################\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "s = 4\n",
    "\n",
    "\n",
    "# def merge_basket(baskets):\n",
    "#     a = list(baskets)\n",
    "#     collected_basket = []\n",
    "#     for i in a:\n",
    "#         collected_basket+=i\n",
    "#     return collected_basket\n",
    "\n",
    "def merge_basket_2d(baskets):\n",
    "    a = list(baskets)\n",
    "    collected_basket = []\n",
    "    for i in a:\n",
    "        collected_basket += [i]\n",
    "    return collected_basket\n",
    "\n",
    "\n",
    "def count_singleton(basket_list_2d):\n",
    "    count_num_dict = {}\n",
    "    for basket in basket_list_2d:\n",
    "        for item in basket:\n",
    "            count_num_dict[item] = count_num_dict.get(item, 0) + 1\n",
    "    return count_num_dict\n",
    "\n",
    "\n",
    "def scaled_down_support_threshold(basket_list_2d, support, entire_size):\n",
    "    partition_size = len(basket_list_2d)\n",
    "    return math.ceil((partition_size / entire_size) * int(support))\n",
    "\n",
    "\n",
    "def get_candidate(count_dict, scaled_down_support_threshold):\n",
    "    candidate_list = []\n",
    "    for i in count_dict.keys():\n",
    "        if count_dict[i] > scaled_down_support_threshold:\n",
    "            candidate_list.append(i)\n",
    "\n",
    "    return candidate_list\n",
    "\n",
    "\n",
    "def get_singleton_candidate_from_paritition(basket_list_2d, ps):\n",
    "    return get_candidate(count_singleton(basket_list_2d), ps)\n",
    "\n",
    "\n",
    "def count_pairs(basket_list_2d, singleton_candidate):\n",
    "    count_dict = {}\n",
    "    for basket in basket_list_2d:\n",
    "        for b1 in basket:\n",
    "            for b2 in basket:\n",
    "                if b1 < b2:\n",
    "                    if b1 in singleton_candidate:\n",
    "                        if b2 in singleton_candidate:\n",
    "                            count_dict[(b1, b2)] = count_dict.get((b1, b2), 0) + 1\n",
    "\n",
    "    return count_dict\n",
    "\n",
    "\n",
    "def get_pair_candidate(basket_list_2d, singleton_candidate, ps):\n",
    "    pair_candi_list = []\n",
    "    pair_count = count_pairs(basket_list_2d, singleton_candidate)\n",
    "    for candidate in pair_count.keys():\n",
    "        if pair_count[candidate] > ps:\n",
    "            pair_candi_list.append(candidate)\n",
    "    return pair_candi_list\n",
    "\n",
    "\n",
    "def generate_permutations(candidate_list, pair_size):\n",
    "    permu_list = []\n",
    "    if len(candidate_list) > 0:\n",
    "        for idx, itemset in enumerate(candidate_list[:-1]):\n",
    "            # print(\"idx, itemset : \", idx, itemset)\n",
    "            for itemset2 in candidate_list[idx + 1:]:\n",
    "                # print(\"itemset2 : \", itemset2)\n",
    "                if list(itemset[:-1]) == list(itemset2[:-1]):\n",
    "                    # print(itemset2[-1])\n",
    "                    # print(sorted(list(itemset) + [itemset2[-1]]))\n",
    "                    candi = sorted(list(itemset) + [itemset2[-1]])\n",
    "\n",
    "                    if candi not in permu_list:\n",
    "                        check_list = [tuple(sorted(i)) for i in list(combinations(candi, pair_size - 1))]\n",
    "                        if set(check_list).issubset(candidate_list):\n",
    "                            permu_list.append(tuple(candi))\n",
    "\n",
    "    return permu_list\n",
    "\n",
    "\n",
    "def apriori(partition, support, entire_size):\n",
    "    total_freq_itemsets = []\n",
    "\n",
    "    basket_list_2d = merge_basket_2d(partition)\n",
    "    #     max_len = max([len(basket) for basket in basket_list_2d])\n",
    "    ps = scaled_down_support_threshold(basket_list_2d, support, entire_size)\n",
    "\n",
    "    ## get singleton candidate\n",
    "    singleton_candidate = sorted(get_singleton_candidate_from_paritition(basket_list_2d, ps))\n",
    "    total_freq_itemsets += singleton_candidate\n",
    "    ## get pair candidate\n",
    "    pair_candidate = sorted(get_pair_candidate(basket_list_2d, singleton_candidate, ps))\n",
    "    total_freq_itemsets += pair_candidate\n",
    "\n",
    "    ## get candidate until no element left\n",
    "    pair_size = 2\n",
    "    candidate_list = pair_candidate\n",
    "    while len(candidate_list) > 0:\n",
    "        pair_size += 1  ## pair_size = 3\n",
    "        count_dict = {}\n",
    "        pairs_to_be_counted = generate_permutations(candidate_list, pair_size)\n",
    "        for pair in pairs_to_be_counted:\n",
    "            for bucket in basket_list_2d:\n",
    "                if set(pair).issubset(bucket):\n",
    "                    count_dict[pair] = count_dict.get(pair, 0) + 1\n",
    "        candidate_list = []\n",
    "        for candi in count_dict.keys():\n",
    "            if count_dict[candi] > ps:\n",
    "                candidate_list.append(candi)\n",
    "\n",
    "        total_freq_itemsets += candidate_list\n",
    "\n",
    "    return total_freq_itemsets\n",
    "\n",
    "\n",
    "#     return [pair_candidate]\n",
    "\n",
    "textRDD = sc.textFile(input_file, 20)\n",
    "header = textRDD.first()  ## remove header\n",
    "tmp = textRDD.filter(lambda x: x != header)\n",
    "baskets = tmp.map(lambda x: x.split(\",\")).mapValues(lambda x: [x]).reduceByKey(lambda a, b: a + b).mapValues(\n",
    "    lambda x: sorted(list(set(x)))).map(lambda x: x[1])\n",
    "# baskets = tmp.collect()\n",
    "entire_size = tmp.count()\n",
    "\n",
    "num = tmp.mapPartitions(lambda x: [sum(1 for i in x)]).collect() ## number of items in each partition\n",
    "baskets\n",
    "\n",
    "## phase 1\n",
    "tmp = baskets.mapPartitions(lambda partition: apriori(partition, s, entire_size)).map(lambda x: (x, 1)).reduceByKey(\n",
    "    lambda a, b: a)\n",
    "candidate_itemsets = tmp.collect()\n",
    "\n",
    "num = tmp.mapPartitions(lambda x: [sum(1 for i in x)]).collect()  ## number of items in each partition\n",
    "\n",
    "for i in range(len(candidate_itemsets)):\n",
    "    if type(candidate_itemsets[i][0]) == str:\n",
    "        candidate_itemsets[i] = ((candidate_itemsets[i][0],), 1)\n",
    "\n",
    "\n",
    "# phase2\n",
    "def Map2(basket, candidate_itemsets):\n",
    "    answer_list = []\n",
    "    for candidate in candidate_itemsets:\n",
    "        if set(candidate[0]).issubset(set(basket)):\n",
    "            answer_list.append((candidate[0], 1))\n",
    "    size = len(answer_list)\n",
    "\n",
    "    return answer_list\n",
    "\n",
    "\n",
    "freq_itemsets = baskets.map(lambda x: Map2(x, candidate_itemsets)).flatMap(lambda x: x).reduceByKey(\n",
    "    lambda a, b: a + b).filter(lambda x: x[1] > int(s)).map(lambda x: x[0]).collect()\n",
    "\n",
    "freq_itemsets = sorted(freq_itemsets, key=lambda x: (len(x), x))\n",
    "\n",
    "freq_itemsets\n",
    "#     return answer_list[0], answer_list[1], ... answer_list[-1]\n",
    "\n",
    "## end time\n",
    "time_spent = (time.time() - start_time)\n",
    "\n",
    "time_spent\n",
    "print(\"##############################\")\n",
    "print(time_spent)\n",
    "print(num)\n",
    "print(\"##############################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('100',),\n",
       " ('101',),\n",
       " ('102',),\n",
       " ('97',),\n",
       " ('98',),\n",
       " ('99',),\n",
       " ('100', '101'),\n",
       " ('101', '99'),\n",
       " ('102', '98'),\n",
       " ('97', '98'),\n",
       " ('97', '99')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "1.5872082710266113\n",
      "[0, 0, 0, 2, 0, 2, 4, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 3, 1, 0, 1]\n",
      "##############################\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "s = 4\n",
    "\n",
    "\n",
    "# def merge_basket(baskets):\n",
    "#     a = list(baskets)\n",
    "#     collected_basket = []\n",
    "#     for i in a:\n",
    "#         collected_basket+=i\n",
    "#     return collected_basket\n",
    "\n",
    "def merge_basket_2d(baskets):\n",
    "    a = list(baskets)\n",
    "    collected_basket = []\n",
    "    for i in a:\n",
    "        collected_basket += [i]\n",
    "    return collected_basket\n",
    "\n",
    "\n",
    "def count_singleton(basket_list_2d):\n",
    "    count_num_dict = {}\n",
    "    for basket in basket_list_2d:\n",
    "        for item in basket:\n",
    "            count_num_dict[item] = count_num_dict.get(item, 0) + 1\n",
    "    return count_num_dict\n",
    "\n",
    "\n",
    "def scaled_down_support_threshold(basket_list_2d, support, entire_size):\n",
    "    partition_size = len(basket_list_2d)\n",
    "    return math.ceil((partition_size / entire_size) * int(support))\n",
    "\n",
    "\n",
    "def get_candidate(count_dict, scaled_down_support_threshold):\n",
    "    candidate_list = []\n",
    "    for i in count_dict.keys():\n",
    "        if count_dict[i] > scaled_down_support_threshold:\n",
    "            candidate_list.append(i)\n",
    "\n",
    "    return candidate_list\n",
    "\n",
    "\n",
    "def get_singleton_candidate_from_paritition(basket_list_2d, ps):\n",
    "    return get_candidate(count_singleton(basket_list_2d), ps)\n",
    "\n",
    "\n",
    "def count_pairs(basket_list_2d, singleton_candidate):\n",
    "    count_dict = {}\n",
    "    for basket in basket_list_2d:\n",
    "        for b1 in basket:\n",
    "            for b2 in basket:\n",
    "                if b1 < b2:\n",
    "                    if b1 in singleton_candidate:\n",
    "                        if b2 in singleton_candidate:\n",
    "                            count_dict[(b1, b2)] = count_dict.get((b1, b2), 0) + 1\n",
    "\n",
    "    return count_dict\n",
    "\n",
    "\n",
    "def get_pair_candidate(basket_list_2d, singleton_candidate, ps):\n",
    "    pair_candi_list = []\n",
    "    pair_count = count_pairs(basket_list_2d, singleton_candidate)\n",
    "    for candidate in pair_count.keys():\n",
    "        if pair_count[candidate] > ps:\n",
    "            pair_candi_list.append(candidate)\n",
    "    return pair_candi_list\n",
    "\n",
    "\n",
    "def generate_permutations(candidate_list, pair_size):\n",
    "    permu_list = []\n",
    "    if len(candidate_list) > 0:\n",
    "        for idx, itemset in enumerate(candidate_list[:-1]):\n",
    "            # print(\"idx, itemset : \", idx, itemset)\n",
    "            for itemset2 in candidate_list[idx + 1:]:\n",
    "                # print(\"itemset2 : \", itemset2)\n",
    "                if list(itemset[:-1]) == list(itemset2[:-1]):\n",
    "                    # print(itemset2[-1])\n",
    "                    # print(sorted(list(itemset) + [itemset2[-1]]))\n",
    "                    candi = sorted(list(itemset) + [itemset2[-1]])\n",
    "\n",
    "                    if candi not in permu_list:\n",
    "                        check_list = [tuple(sorted(i)) for i in list(combinations(candi, pair_size - 1))]\n",
    "                        if set(check_list).issubset(candidate_list):\n",
    "                            permu_list.append(tuple(candi))\n",
    "\n",
    "    return permu_list\n",
    "\n",
    "\n",
    "def apriori(partition, support, entire_size):\n",
    "    total_freq_itemsets = []\n",
    "\n",
    "    basket_list_2d = merge_basket_2d(partition)\n",
    "    #     max_len = max([len(basket) for basket in basket_list_2d])\n",
    "    ps = scaled_down_support_threshold(basket_list_2d, support, entire_size)\n",
    "\n",
    "    ## get singleton candidate\n",
    "    singleton_candidate = sorted(get_singleton_candidate_from_paritition(basket_list_2d, ps))\n",
    "    total_freq_itemsets += singleton_candidate\n",
    "    ## get pair candidate\n",
    "    pair_candidate = sorted(get_pair_candidate(basket_list_2d, singleton_candidate, ps))\n",
    "    total_freq_itemsets += pair_candidate\n",
    "\n",
    "    ## get candidate until no element left\n",
    "    pair_size = 2\n",
    "    candidate_list = pair_candidate\n",
    "    while len(candidate_list) > 0:\n",
    "        pair_size += 1  ## pair_size = 3\n",
    "        count_dict = {}\n",
    "        pairs_to_be_counted = generate_permutations(candidate_list, pair_size)\n",
    "        for pair in pairs_to_be_counted:\n",
    "            for bucket in basket_list_2d:\n",
    "                if set(pair).issubset(bucket):\n",
    "                    count_dict[pair] = count_dict.get(pair, 0) + 1\n",
    "        candidate_list = []\n",
    "        for candi in count_dict.keys():\n",
    "            if count_dict[candi] > ps:\n",
    "                candidate_list.append(candi)\n",
    "\n",
    "        total_freq_itemsets += candidate_list\n",
    "\n",
    "    return total_freq_itemsets\n",
    "\n",
    "\n",
    "#     return [pair_candidate]\n",
    "\n",
    "textRDD = sc.textFile(input_file, 20)\n",
    "header = textRDD.first()  ## remove header\n",
    "tmp = textRDD.filter(lambda x: x != header)\n",
    "baskets = tmp.map(lambda x: x.split(\",\")).mapValues(lambda x: [x]).reduceByKey(lambda a, b: a + b).mapValues(\n",
    "    lambda x: sorted(list(set(x)))).map(lambda x: x[1])\n",
    "# baskets = tmp.collect()\n",
    "entire_size = tmp.count()\n",
    "\n",
    "num = tmp.mapPartitions(lambda x: [sum(1 for i in x)]).collect() ## number of items in each partition\n",
    "baskets\n",
    "\n",
    "## phase 1\n",
    "tmp = baskets.mapPartitions(lambda partition: apriori(partition, s, entire_size)).map(lambda x: (x, 1)).reduceByKey(\n",
    "    lambda a, b: a)\n",
    "candidate_itemsets = tmp.collect()\n",
    "\n",
    "num = tmp.mapPartitions(lambda x: [sum(1 for i in x)]).collect()  ## number of items in each partition\n",
    "\n",
    "for i in range(len(candidate_itemsets)):\n",
    "    if type(candidate_itemsets[i][0]) == str:\n",
    "        candidate_itemsets[i] = ((candidate_itemsets[i][0],), 1)\n",
    "\n",
    "\n",
    "# phase2\n",
    "def Map2(basket, candidate_itemsets):\n",
    "    answer_list = []\n",
    "    for candidate in candidate_itemsets:\n",
    "        if set(candidate[0]).issubset(set(basket)):\n",
    "            answer_list.append((candidate[0], 1))\n",
    "    size = len(answer_list)\n",
    "\n",
    "    return answer_list\n",
    "\n",
    "\n",
    "freq_itemsets = baskets.map(lambda x: Map2(x, candidate_itemsets)).flatMap(lambda x: x).reduceByKey(\n",
    "    lambda a, b: a + b).filter(lambda x: x[1] > int(s)).map(lambda x: x[0]).collect()\n",
    "\n",
    "freq_itemsets = sorted(freq_itemsets, key=lambda x: (len(x), x))\n",
    "\n",
    "freq_itemsets\n",
    "#     return answer_list[0], answer_list[1], ... answer_list[-1]\n",
    "\n",
    "## end time\n",
    "time_spent = (time.time() - start_time)\n",
    "\n",
    "time_spent\n",
    "print(\"##############################\")\n",
    "print(time_spent)\n",
    "print(num)\n",
    "print(\"##############################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('97',), 1),\n",
       " (('98',), 1),\n",
       " (('100', '99'), 1),\n",
       " (('97', '98'), 1),\n",
       " (('99',), 1),\n",
       " (('100', '101', '99'), 1),\n",
       " (('102',), 1),\n",
       " (('102', '105', '98'), 1),\n",
       " (('100',), 1),\n",
       " (('105',), 1),\n",
       " (('101',), 1),\n",
       " (('105', '98'), 1),\n",
       " (('101', '99'), 1),\n",
       " (('102', '105'), 1),\n",
       " (('102', '98'), 1),\n",
       " (('97', '99'), 1),\n",
       " (('100', '101'), 1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
