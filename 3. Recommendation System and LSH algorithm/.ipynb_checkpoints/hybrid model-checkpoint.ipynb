{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "import json\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "import sys\n",
    "from pyspark import SparkContext \n",
    "from sklearn import preprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext('local[*]', 'task2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/Users/jaeyoungkim/Desktop/usc/DSCI - 553/hw/hw3/\"\n",
    "test_file_name = \"yelp_val_in.csv\"\n",
    "output_file = \"task2_3.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_ftn(user_id, business_id, prediction_CF, prediction_model, num, user_dict, business_dict):\n",
    "    if prediction_CF == -1:\n",
    "        return prediction_model\n",
    "    else:\n",
    "        user_review_count = user_dict[user_id][\"n_review_review\"]\n",
    "        business_review_count = business_dict[business_id][\"n_review_review\"]\n",
    "        if num< 100:\n",
    "            return prediction_model\n",
    "        else:   \n",
    "            model_count = (user_review_count+business_review_count)/2\n",
    "            a = (num/(model_count+num))*0.3\n",
    "            return a * prediction_CF + (1-a) * prediction_model\n",
    "        \n",
    "def len_dict(x):\n",
    "    if x == \"None\":\n",
    "        return 0\n",
    "    elif x == None:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(x)\n",
    "    \n",
    "def date_cal(x):\n",
    "#     print(x)\n",
    "    return (datetime.date(2021,3,10)-datetime.date(int(x.split(\"-\")[0]),int(x.split(\"-\")[1]),int(x.split(\"-\")[2]))).days\n",
    "\n",
    "def get_length(x):\n",
    "    if x ==\"None\":\n",
    "        return 0\n",
    "    elif x == None:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(x.split(\",\"))\n",
    "\n",
    "def get_length_1(x):\n",
    "    if x ==\"None\":\n",
    "        return 0\n",
    "    elif x == None:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(x)\n",
    "    \n",
    "def get_max(x):\n",
    "    if x == \"None\":\n",
    "        return 0\n",
    "    elif x == None:\n",
    "        return 0\n",
    "    else : \n",
    "        return int(sorted(x.split(\",\"), reverse = True)[0])\n",
    "        \n",
    "def avg(x):\n",
    "    a =0\n",
    "    for i in x:\n",
    "        a+=i\n",
    "    return a/len(x)\n",
    "def summation(x):\n",
    "    a =0\n",
    "    for i in x:\n",
    "        a+=i\n",
    "    return a\n",
    "\n",
    "def dict_merge(a,b):\n",
    "    a.update(b)\n",
    "    return a\n",
    "\n",
    "def similarity(business1, business2):\n",
    "    a = business_basket_dict[business1]\n",
    "    b = business_basket_dict[business2]\n",
    "    \n",
    "    if len(a) ==0 or len(b) ==0:\n",
    "        return 0\n",
    "    else:\n",
    "        avg_a = sum(a.values())/len(a)\n",
    "        avg_b = sum(b.values())/len(b)\n",
    "\n",
    "        div_a = sum([(i-avg_a)**2 for i in a.values()])**0.5\n",
    "        div_b = sum([(i-avg_b)**2 for i in b.values()])**0.5\n",
    "\n",
    "        num = 0\n",
    "        for i in set(a.keys()).intersection(set(b.keys())):\n",
    "            num += (a[i]-avg_a)*(b[i]-avg_b)\n",
    "\n",
    "        if div_a ==0 or div_b == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return num/(div_a*div_b)+0.1 ## to remove negative similarities\n",
    "\n",
    "    \n",
    "def score(x, user):  ## neighborhood 2\n",
    "    num = 0\n",
    "    div = 0\n",
    "    x = sorted(x, key=lambda a: -a[0])\n",
    "    count = 0\n",
    "    while count<3:\n",
    "        count+=1\n",
    "        for a,b in x: ## w, r\n",
    "            if a !=0:\n",
    "                num += a*b\n",
    "                div += abs(a)\n",
    "    if div == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return num/div\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "   \n",
    "#     input_file = sys.argv[1]\n",
    "#     output_file = sys.argv[2]\n",
    "    \n",
    "    t = time.time()\n",
    "    n = 30\n",
    "    \n",
    "\n",
    "    \n",
    "    ## RDDs\n",
    "    textRDD = sc.textFile(folder_path + 'yelp_train.csv', n)\n",
    "    testRDD = sc.textFile(test_file_name, n)\n",
    "    \n",
    "    ## exclue header\n",
    "    header = textRDD.first()  ## remove header\n",
    "    header2 = testRDD.first()  ## remove header\n",
    "    tmp_train = textRDD.filter(lambda x: x != header)\n",
    "    tmp_test = testRDD.filter(lambda x: x!= header2)\n",
    "    \n",
    "    \n",
    "    ##make dictionary for user and business\n",
    "    user_list_train = tmp_train.map(lambda x : (x.split(\",\")[0],1)).reduceByKey(lambda a,b : a).map(lambda x : (1,[x[0]])).reduceByKey(lambda a,b : a+b).collect()[0][1]\n",
    "    user_list_test = tmp_test.map(lambda x : (x.split(\",\")[0],1)).reduceByKey(lambda a,b : a).map(lambda x : (1,[x[0]])).reduceByKey(lambda a,b : a+b).collect()[0][1]\n",
    "    user_list = list(set(user_list_train+user_list_test))\n",
    "    user_to_idx = {}\n",
    "    for idx, user in enumerate(user_list):\n",
    "        user_to_idx[user] = idx\n",
    "        \n",
    "    business_list_train = tmp_train.map(lambda x : (x.split(\",\")[1],1)).reduceByKey(lambda a,b : a).map(lambda x : (1,[x[0]])).reduceByKey(lambda a,b : a+b).collect()[0][1]\n",
    "    business_list_test = tmp_test.map(lambda x : (x.split(\",\")[1],1)).reduceByKey(lambda a,b : a).map(lambda x : (1,[x[0]])).reduceByKey(lambda a,b : a+b).collect()[0][1]\n",
    "    business_list = list(set(business_list_train+business_list_test))\n",
    "    business_to_idx = {}\n",
    "    for idx, business in enumerate(business_list):\n",
    "        business_to_idx[business] = idx\n",
    "        \n",
    "    ###################################\n",
    "    ##### Collaborative Filtering #####\n",
    "    ###################################\n",
    "        \n",
    "    ## user basket\n",
    "    \n",
    "    user_basket_dict = tmp_train.map(lambda x : (user_to_idx[x.split(\",\")[0]], {business_to_idx[x.split(\",\")[1]]: float(x.split(\",\")[2])}))\\\n",
    "    .reduceByKey(lambda a,b : dict_merge(a,b)).map(lambda x : (1,{x[0]:x[1]})).reduceByKey(lambda a,b : dict_merge(a,b)).map(lambda x : x[1]).collect()[0]\n",
    "    \n",
    "    for user_id in user_to_idx.values():\n",
    "        if user_id not in user_basket_dict.keys():\n",
    "            user_basket_dict[user_id] = {}\n",
    "        \n",
    "    print(\"Duration : \", time.time()-t)\n",
    "\n",
    "    ## business basket\n",
    "    \n",
    "    business_basket_dict = tmp_train.map(lambda x : (business_to_idx[x.split(\",\")[1]], {user_to_idx[x.split(\",\")[0]]: float(x.split(\",\")[2])}))\\\n",
    "    .reduceByKey(lambda a,b : dict_merge(a,b)).map(lambda x : (1,{x[0]:x[1]})).reduceByKey(lambda a,b : dict_merge(a,b)).map(lambda x : x[1]).collect()[0]\n",
    "    print(\"Duration : \", time.time()-t)\n",
    "    \n",
    "    for business_id in business_to_idx.values():\n",
    "        if business_id not in business_basket_dict.keys():\n",
    "            business_basket_dict[business_id] = {}\n",
    "            \n",
    "    ## id to user, id to business\n",
    "    \n",
    "    idx_to_user = {v: k for k, v in user_to_idx.items()}\n",
    "    idx_to_business = {v: k for k, v in business_to_idx.items()}\n",
    "    \n",
    "    print(\"Duration : \", time.time()-t)\n",
    "    \n",
    "    \n",
    "    val = tmp_test.map(lambda x : (user_to_idx[x.split(\",\")[0]], business_to_idx[x.split(\",\")[1]]))\\\n",
    "        .map(lambda x : (x[0],x[1], user_basket_dict[x[0]]))\\\n",
    "        .map(lambda x : (x[0],x[1],[(similarity(x[1], k), v) for k,v in x[2].items()], get_length_1(x[2])))\\\n",
    "        .map(lambda x : (idx_to_user[x[0]],idx_to_business[x[1]],score(x[2], x[0]), x[3]))\n",
    "     \n",
    "    answer_CF = val.collect()\n",
    "\n",
    "    #################################\n",
    "    ##### Model Based Filtering #####\n",
    "    #################################\n",
    "\n",
    "    ##create user_dictionary, business_dictionary\n",
    "        \n",
    "    business_dict = {}\n",
    "    user_dict = {}\n",
    "    \n",
    "    for user in user_list:\n",
    "        user_dict[user] = {}\n",
    "    for business in business_list:\n",
    "        business_dict[business] = {}\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    ### fill user_dict and business_dict with json files\n",
    "    '''\n",
    "    checkin : checkin sum, avg for business\n",
    "\n",
    "    business : stars, review_count, latitude, longitude, how many days does it open (len(hours)), attribute length?, \n",
    "\n",
    "    photo  : number of photos\n",
    "\n",
    "    review : number of reviews based on user, number of reviews based on business, avg star, useful (sum avg), funny (sum, avg), cool (sum, avg)\n",
    "\n",
    "    tip : number of tips based on user, number of tips based on business, number of likes\n",
    "\n",
    "    user : review_count, daycount of (today -yelpsince), number of friend , useful, funny, cool, fans, most reent \"elite\" year, number of \"elite\", avgstars, \n",
    "\n",
    "    compliment_hot, more, profile, cute, list, note plain, cool, funny, writer, photos\n",
    "    \n",
    "    '''\n",
    "    ### Extract Features and then merge it into user_dict and busines_dict\n",
    "    \n",
    "    ## checkin\n",
    "    checkinRDD = sc.textFile(folder_path + \"checkin.json\", n)\n",
    "    checkin = checkinRDD.map(lambda line : json.loads(line)).map(lambda x : (x[\"business_id\"], x[\"time\"])).map(lambda x: (x[0], list(x[1].values()))).map(lambda x : (x[0], summation(x[1]),avg(x[1]))).collect()\n",
    "    for i in checkin:\n",
    "        try:\n",
    "            business_dict[i[0]][\"checkin_sum\"] = i[1]\n",
    "            business_dict[i[0]][\"checkin_avg\"] = i[2]\n",
    "        except:\n",
    "            continue\n",
    "    del checkin\n",
    "    \n",
    "    # business\n",
    "    businessRDD = sc.textFile(folder_path + \"business.json\", n)\n",
    "    business = businessRDD.map(lambda line : json.loads(line))\\\n",
    "    .map(lambda x : (x[\"business_id\"], x[\"latitude\"], x[\"longitude\"], x[\"stars\"], x[\"review_count\"], x[\"is_open\"], len_dict(x[\"attributes\"]), get_length(x[\"categories\"]), len_dict(x[\"hours\"]))).collect()\n",
    "\n",
    "\n",
    "    for i in business:\n",
    "        try:\n",
    "            business_dict[i[0]][\"latitude\"] = i[1]\n",
    "            business_dict[i[0]][\"longitude\"] = i[2]\n",
    "            business_dict[i[0]][\"stars\"] = i[3]\n",
    "            business_dict[i[0]][\"review_count\"] = i[4]\n",
    "            business_dict[i[0]][\"is_open\"] = i[5]\n",
    "            business_dict[i[0]][\"len_attributes\"] = i[6]\n",
    "            business_dict[i[0]][\"len_categories\"] = i[7]\n",
    "            business_dict[i[0]][\"len_hours\"] = i[8]\n",
    "        except:\n",
    "            continue\n",
    "    del business\n",
    "            \n",
    "    \n",
    "    # photo\n",
    "    photoRDD = sc.textFile(folder_path + \"photo.json\", n)\n",
    "    photo = photoRDD.map(lambda line : json.loads(line))\\\n",
    "            .map(lambda x: (x[\"business_id\"], 1)).reduceByKey(lambda a,b : a+b).collect()\n",
    "    # .map(lambda x : (x[\"business_id\"], x[\"latitude\"], x[\"longitude\"], x[\"stars\"], x[\"review_count\"], x[\"is_open\"], len_dict(x[\"attributes\"]), get_length(x[\"categories\"]), len_dict(x[\"hours\"]))).collect()\n",
    "\n",
    "\n",
    "    for i in photo:\n",
    "        try:\n",
    "            business_dict[i[0]][\"n_photo\"] = i[1]\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    del photo\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # review\n",
    "    reviewRDD = sc.textFile(folder_path + \"review_train.json\", n)\n",
    "    review = reviewRDD.map(lambda line : json.loads(line))\\\n",
    "            .map(lambda x: (x[\"business_id\"], (1, x[\"stars\"], len(x[\"text\"]) ))).reduceByKey(lambda a,b : (a[0]+b[0], a[1]+b[1], a[2]+b[2]))\\\n",
    "    .map(lambda x: (x[0], x[1][0], x[1][1]/x[1][0], x[1][2]/x[1][0])).collect()\n",
    "\n",
    "    for i in review:\n",
    "        try:\n",
    "            business_dict[i[0]][\"n_review_review\"] = i[1]\n",
    "            business_dict[i[0]][\"avg_review_stars\"] = i[2]\n",
    "            business_dict[i[0]][\"avg_review_len\"] = i[3]\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "    review = reviewRDD.map(lambda line : json.loads(line))\\\n",
    "            .map(lambda x: (x[\"user_id\"], (1, x[\"stars\"], len(x[\"text\"]) ))).reduceByKey(lambda a,b : (a[0]+b[0], a[1]+b[1], a[2]+b[2]))\\\n",
    "    .map(lambda x: (x[0], x[1][0], x[1][1]/x[1][0], x[1][2]/x[1][0])).collect()\n",
    "\n",
    "    for i in review:\n",
    "        try:\n",
    "            user_dict[i[0]][\"n_review_review\"] = i[1]\n",
    "            user_dict[i[0]][\"avg_review_stars\"] = i[2]\n",
    "            user_dict[i[0]][\"avg_review_len\"] = i[3]\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    del review\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # tip\n",
    "    tipRDD = sc.textFile(folder_path + \"tip.json\", n)\n",
    "    tip = tipRDD.map(lambda line : json.loads(line))\\\n",
    "            .map(lambda x: (x[\"business_id\"], (1, x[\"likes\"], len(x[\"text\"]) ))).reduceByKey(lambda a,b : (a[0]+b[0], a[1]+b[1], a[2]+b[2]))\\\n",
    "            .map(lambda x: (x[0], x[1][0], x[1][1]/x[1][0], x[1][2]/x[1][0])).collect()\n",
    "\n",
    "\n",
    "    for i in tip:\n",
    "        try:\n",
    "            business_dict[i[0]][\"n_tip_business\"] = i[1]\n",
    "            business_dict[i[0]][\"avg_like_business\"] = i[2]\n",
    "            business_dict[i[0]][\"avg_tip_len_business\"] = i[3]\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    tip = tipRDD.map(lambda line : json.loads(line))\\\n",
    "            .map(lambda x: (x[\"user_id\"], (1, x[\"likes\"], len(x[\"text\"]) ))).reduceByKey(lambda a,b : (a[0]+b[0], a[1]+b[1], a[2]+b[2]))\\\n",
    "            .map(lambda x: (x[0], x[1][0], x[1][1]/x[1][0], x[1][2]/x[1][0])).collect()\n",
    "\n",
    "\n",
    "    for i in tip:\n",
    "        try:\n",
    "            user_dict[i[0]][\"n_tip_user\"] = i[1]\n",
    "            user_dict[i[0]][\"avg_like_user\"] = i[2]\n",
    "            user_dict[i[0]][\"avg_tip_len_user\"] = i[3]\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    del tip\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## user\n",
    "    userRDD = sc.textFile(folder_path + \"user.json\", n).\\\n",
    "        map(lambda line : json.loads(line))\\\n",
    "        .filter(lambda x : x[\"user_id\"] in user_list)\n",
    "    user_data = userRDD.map(lambda x : (x[\"user_id\"], x[\"review_count\"],\\\n",
    "                     (datetime.date(2021,3,10)-datetime.date(int(x[\"yelping_since\"].split(\"-\")[0]),int(x[\"yelping_since\"].split(\"-\")[1]),int(x[\"yelping_since\"].split(\"-\")[2]))).days,\\\n",
    "                     get_length(x[\"friends\"]),\\\n",
    "                     x[\"useful\"],\\\n",
    "                    x[\"funny\"],\\\n",
    "                    x[\"fans\"],\\\n",
    "                    get_length(x[\"elite\"]),\\\n",
    "                    get_max(x[\"elite\"]),\\\n",
    "                    x[\"average_stars\"],\\\n",
    "                    x[\"compliment_hot\"],\\\n",
    "                    x[\"compliment_more\"],\\\n",
    "                    x[\"compliment_cute\"],\\\n",
    "                     x[\"compliment_list\"],\\\n",
    "                     x[\"compliment_note\"],\\\n",
    "                     x[\"compliment_plain\"],\\\n",
    "                     x[\"compliment_cool\"],\\\n",
    "                     x[\"compliment_funny\"],\\\n",
    "                     x[\"compliment_writer\"],\\\n",
    "                     x[\"compliment_photos\"],\\\n",
    "                    )).collect()\n",
    "\n",
    "    \n",
    "    \n",
    "    for i in user_data:\n",
    "        try:\n",
    "            user_dict[i[0]][\"review_count\"] = i[1]\n",
    "            user_dict[i[0]][\"date_since\"] = i[2]\n",
    "            user_dict[i[0]][\"n_friends\"] = i[3]\n",
    "            user_dict[i[0]][\"useful\"] = i[4]\n",
    "            user_dict[i[0]][\"funny\"] = i[5]\n",
    "            user_dict[i[0]][\"fans\"] = i[6]\n",
    "            user_dict[i[0]][\"n_elite\"] = i[7]\n",
    "            user_dict[i[0]][\"max_elite\"] = i[8]\n",
    "            user_dict[i[0]][\"avg_stars\"] = i[9]\n",
    "            user_dict[i[0]][\"compliment_hot\"] = i[10]\n",
    "            user_dict[i[0]][\"compliment_more\"] = i[11]\n",
    "            user_dict[i[0]][\"compliment_cute\"] = i[12]\n",
    "            user_dict[i[0]][\"compliment_list\"] = i[13]\n",
    "            user_dict[i[0]][\"compliment_note\"] = i[14]\n",
    "            user_dict[i[0]][\"compliment_plain\"] = i[15]\n",
    "            user_dict[i[0]][\"compliment_cool\"] = i[16]\n",
    "            user_dict[i[0]][\"compliment_funny\"] = i[17]\n",
    "            user_dict[i[0]][\"compliment_writer\"] = i[18]\n",
    "            user_dict[i[0]][\"compliment_photos\"] = i[19]\n",
    "            \n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "    del user_data\n",
    "            \n",
    "    print(\"Duration : \", time.time()-t)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## make train_data from business_dict and user_dict\n",
    "    ## \n",
    "    \n",
    "    train_vec = tmp_train.map(lambda x: (x.split(\",\")[0],x.split(\",\")[1], x.split(\",\")[2]))\\\n",
    "        .map(lambda x : (x[0], x[1], user_dict[x[0]],business_dict[x[1]], float(x[2])))\\\n",
    "    #         .map(lambda x : (x[0], x[1], x[2].update(x[3])))\n",
    "    train_vec = train_vec.collect()\n",
    "    print(\"Duration : \", time.time()-t)\n",
    "    \n",
    "\n",
    "    train_data = pd.DataFrame()\n",
    "    data_row = []\n",
    "    count =0\n",
    "    for train_row in train_vec:\n",
    "        count+=1\n",
    "        row = {}\n",
    "        for i in train_row[2].items():\n",
    "            row[i[0]] = i[1]\n",
    "        for i in train_row[3].items():\n",
    "            row[i[0]] = i[1]\n",
    "        row[\"target\"] = train_row[4]\n",
    "        data_row.append(row)\n",
    "\n",
    "    train_data = pd.DataFrame.from_dict(data_row)\n",
    "    print(\"Duration : \", time.time()-t)\n",
    "\n",
    "    \n",
    "    ## minmax scaler\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    train_data_no_label = train_data.loc[:,train_data.columns != 'target']\n",
    "    train_data_no_label_scaled = pd.DataFrame(scaler.fit_transform(train_data_no_label), index = train_data_no_label.index, columns = train_data_no_label.columns)\n",
    "    train_data_no_label_scaled[\"target\"] =  train_data[\"target\"]\n",
    "    train_data  = train_data_no_label_scaled\n",
    "    train_data.fillna(-1)\n",
    "    \n",
    "    \n",
    "    ## make test vector\n",
    "    test_vec = tmp_test.map(lambda x: (x.split(\",\")[0],x.split(\",\")[1]))\\\n",
    "            .map(lambda x : (x[0], x[1], user_dict[x[0]],business_dict[x[1]]))\n",
    "    test_vec = test_vec.collect()## user , business\n",
    "    \n",
    "    \n",
    "    test_data = pd.DataFrame()\n",
    "    data_row = []\n",
    "    count =0\n",
    "    for test_row in test_vec:\n",
    "        count+=1\n",
    "        row = {}\n",
    "        for i in test_row[2].items():\n",
    "            row[i[0]] = i[1]\n",
    "        for i in test_row[3].items():\n",
    "            row[i[0]] = i[1]\n",
    "#         row[\"target\"] = train_row[4]\n",
    "        data_row.append(row)\n",
    "\n",
    "    test_data = pd.DataFrame.from_dict(data_row)\n",
    "    test_data = test_data.fillna(-1)\n",
    "    \n",
    "    ## minmax scaler\n",
    "    test_data = pd.DataFrame(scaler.transform(test_data), index=test_data.index, columns=test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:01:10] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[18:01:27] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Xgb train Duration :  9350.773019075394\n",
      "Duration :  9350.773291110992\n"
     ]
    }
   ],
   "source": [
    "    ## train xgboost regressor\n",
    "#     xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "#                 max_depth = 10, alpha = 0, n_estimators = 100, random_state = 0)\n",
    "    xg_reg =xgb.XGBRegressor(max_depth = 10, n_estimators = 100, objective = 'reg:linear', colsample_bytree = 0.3, learning_rate = 0.1)\n",
    "    selected_columns = ['stars',\n",
    "  'avg_review_stars',\n",
    "  'avg_stars',\n",
    "  'len_hours',\n",
    "  'is_open',\n",
    "  'n_elite',\n",
    "  'compliment_photos',\n",
    "  'compliment_hot',\n",
    "  'compliment_funny',\n",
    "  'n_friends',\n",
    "  'max_elite',\n",
    "  'latitude']\n",
    "    xg_reg.fit(train_data.loc[:, train_data.columns != 'target'][selected_columns],train_data[\"target\"])\n",
    "    print(\"Xgb train Duration : \", time.time() - t)\n",
    "    \n",
    "\n",
    "    \n",
    "    print(\"Duration : \", time.time()-t)\n",
    "    \n",
    "    predictions = xg_reg.predict(test_data[selected_columns])\n",
    "    \n",
    "\n",
    "    answer_model = []\n",
    "    for test , score in zip(test_vec, predictions):\n",
    "        answer_model.append((test[0], test[1], score))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration :  9359.957593917847\n"
     ]
    }
   ],
   "source": [
    "    CF_pandas = pd.DataFrame(answer_CF, columns = [\"user_id\", \"business_id\", \"prediction_CF\", \"num\"])\n",
    "    model_pandas = pd.DataFrame(answer_model, columns = [\"user_id\", \"business_id\", \"prediction_model\"])\n",
    "    merged_df = pd.merge(CF_pandas, model_pandas,  how='left', left_on=['user_id','business_id'], right_on = ['user_id','business_id'])\n",
    "    merged_df[\"prediction\"] = merged_df.apply(lambda x : prediction_ftn(x[\"user_id\"], x[\"business_id\"], x[\"prediction_CF\"], x[\"prediction_model\"], x[\"num\"], user_dict, business_dict), axis = 1)\n",
    "    answer = merged_df[[\"user_id\", \"business_id\", \"prediction\"]]\n",
    "    answer = answer.set_index(\"user_id\")\n",
    "    answer.to_csv(output_file, sep = ',')\n",
    "    \n",
    "\n",
    "    print(\"Duration : \", time.time() - t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer.to_csv(\"task2_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :  0.9885197985961\n"
     ]
    }
   ],
   "source": [
    "res = pd.read_csv(\"task2_3.csv\")\n",
    "val = pd.read_csv(\"yelp_val.csv\")\n",
    "\n",
    "new_df = pd.merge(res, val,  how='left', left_on=['user_id','business_id'], right_on = ['user_id','business_id'])\n",
    "\n",
    "new_df[\"rmse\"] = new_df[\"prediction\"]-new_df[\"stars\"]\n",
    "\n",
    "new_df[\"rmse\"] = new_df.rmse.apply(lambda x : x**2)\n",
    "\n",
    "print(\"RMSE : \",(sum(new_df.rmse)/142043)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wf1GqnKQuvH-V3QN80UOOQ</td>\n",
       "      <td>fThrN4tfupIGetkrz18JOg</td>\n",
       "      <td>3.870688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39FT2Ui8KUXwmUt6hnwy-g</td>\n",
       "      <td>uW6UHfONAmm8QttPkbMewQ</td>\n",
       "      <td>4.914444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7weuSPSSqYLUFga6IYP4pg</td>\n",
       "      <td>IhNASEZ3XnBHmuuVnWdIwA</td>\n",
       "      <td>4.771844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CqaIzLiWaa-lMFYBAsYQxw</td>\n",
       "      <td>G859H6xfAmVLxbzQgipuoA</td>\n",
       "      <td>4.519258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yy7shAsNWRbGg-8Y67Dzag</td>\n",
       "      <td>rS39YnrhoXmPqHLzCBjeqw</td>\n",
       "      <td>2.854908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142039</th>\n",
       "      <td>pA9NXgASl86RImkdBtydrA</td>\n",
       "      <td>q6-SF8zHFU1AWO70k92o1Q</td>\n",
       "      <td>3.354326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142040</th>\n",
       "      <td>_eUb7UGsUoSfi9n2ieF5ow</td>\n",
       "      <td>hgWMxKhrnOUd3m5nOUBIkA</td>\n",
       "      <td>3.055586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142041</th>\n",
       "      <td>cEJGXB63KhROA-XmE_jgXw</td>\n",
       "      <td>0ldxjei8v4q95fApIei3Lg</td>\n",
       "      <td>3.580641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142042</th>\n",
       "      <td>Z4-V0hc51oxUdULWJOufeg</td>\n",
       "      <td>j29tuUdrfaxmGjwxHdHZPA</td>\n",
       "      <td>3.958234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142043</th>\n",
       "      <td>qUL3CdRRF1vedNvaq06rIA</td>\n",
       "      <td>AYL_y8ahquUW0o-cvIyLbg</td>\n",
       "      <td>3.608985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142044 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       user_id             business_id  prediction\n",
       "0       wf1GqnKQuvH-V3QN80UOOQ  fThrN4tfupIGetkrz18JOg    3.870688\n",
       "1       39FT2Ui8KUXwmUt6hnwy-g  uW6UHfONAmm8QttPkbMewQ    4.914444\n",
       "2       7weuSPSSqYLUFga6IYP4pg  IhNASEZ3XnBHmuuVnWdIwA    4.771844\n",
       "3       CqaIzLiWaa-lMFYBAsYQxw  G859H6xfAmVLxbzQgipuoA    4.519258\n",
       "4       yy7shAsNWRbGg-8Y67Dzag  rS39YnrhoXmPqHLzCBjeqw    2.854908\n",
       "...                        ...                     ...         ...\n",
       "142039  pA9NXgASl86RImkdBtydrA  q6-SF8zHFU1AWO70k92o1Q    3.354326\n",
       "142040  _eUb7UGsUoSfi9n2ieF5ow  hgWMxKhrnOUd3m5nOUBIkA    3.055586\n",
       "142041  cEJGXB63KhROA-XmE_jgXw  0ldxjei8v4q95fApIei3Lg    3.580641\n",
       "142042  Z4-V0hc51oxUdULWJOufeg  j29tuUdrfaxmGjwxHdHZPA    3.958234\n",
       "142043  qUL3CdRRF1vedNvaq06rIA  AYL_y8ahquUW0o-cvIyLbg    3.608985\n",
       "\n",
       "[142044 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 0.5 : 0.988827\n",
    "## 0.3 : 0.988253"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.read_csv(\"yelp_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.merge(val, merged_df,  how='left', left_on=['user_id','business_id'], right_on = ['user_id','business_id'])\n",
    "new_df[\"dif\"] = (new_df[\"prediction_CF\"]-new_df[\"stars\"])**2 - (new_df[\"prediction_model\"]-new_df[\"stars\"])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"rmse\"] = (new_df[\"stars\"]-new_df[\"prediction\"])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9881880293264297"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum(new_df[\"rmse\"])/len(new_df)) **0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "answer.to_csv(output_file, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer1 = answer.set_index(\"user_id\")\n",
    "\n",
    "answer1.to_csv(\"task2_3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_ftn(user_id, business_id, prediction_CF, prediction_model, num, user_dict, business_dict):\n",
    "    if prediction_CF == -1:\n",
    "        return prediction_model\n",
    "    else:\n",
    "        user_review_count = user_dict[user_id][\"n_review_review\"]\n",
    "        business_review_count = business_dict[business_id][\"n_review_review\"]\n",
    "        if num< 100:\n",
    "            return prediction_model\n",
    "        else:   \n",
    "            model_count = (user_review_count+business_review_count)/2\n",
    "            a = (num/(model_count+num))*0.3\n",
    "            a = 0\n",
    "            return a * prediction_CF + (1-a) * prediction_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration :  5709.649426937103\n"
     ]
    }
   ],
   "source": [
    "    CF_pandas = pd.DataFrame(answer_CF, columns = [\"user_id\", \"business_id\", \"prediction_CF\", \"num\"])\n",
    "    model_pandas = pd.DataFrame(answer_model, columns = [\"user_id\", \"business_id\", \"prediction_model\"])\n",
    "    merged_df = pd.merge(CF_pandas, model_pandas,  how='left', left_on=['user_id','business_id'], right_on = ['user_id','business_id'])\n",
    "    merged_df[\"prediction\"] = merged_df.apply(lambda x : prediction_ftn(x[\"user_id\"], x[\"business_id\"], x[\"prediction_CF\"], x[\"prediction_model\"], x[\"num\"], user_dict, business_dict), axis = 1)\n",
    "    answer = merged_df[[\"user_id\", \"business_id\", \"prediction\"]]\n",
    "    answer = answer.set_index(\"user_id\")\n",
    "    answer.to_csv(output_file, sep = ',')\n",
    "    \n",
    "\n",
    "    print(\"Duration : \", time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :  0.9885197985961\n"
     ]
    }
   ],
   "source": [
    "res = pd.read_csv(\"task2_3.csv\")\n",
    "val = pd.read_csv(\"yelp_val.csv\")\n",
    "\n",
    "new_df = pd.merge(res, val,  how='left', left_on=['user_id','business_id'], right_on = ['user_id','business_id'])\n",
    "\n",
    "new_df[\"rmse\"] = new_df[\"prediction\"]-new_df[\"stars\"]\n",
    "\n",
    "new_df[\"rmse\"] = new_df.rmse.apply(lambda x : x**2)\n",
    "\n",
    "print(\"RMSE : \",(sum(new_df.rmse)/142043)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>prediction</th>\n",
       "      <th>stars</th>\n",
       "      <th>rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wf1GqnKQuvH-V3QN80UOOQ</td>\n",
       "      <td>fThrN4tfupIGetkrz18JOg</td>\n",
       "      <td>3.870688</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.275345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39FT2Ui8KUXwmUt6hnwy-g</td>\n",
       "      <td>uW6UHfONAmm8QttPkbMewQ</td>\n",
       "      <td>4.914444</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.007320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7weuSPSSqYLUFga6IYP4pg</td>\n",
       "      <td>IhNASEZ3XnBHmuuVnWdIwA</td>\n",
       "      <td>4.886283</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.785498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CqaIzLiWaa-lMFYBAsYQxw</td>\n",
       "      <td>G859H6xfAmVLxbzQgipuoA</td>\n",
       "      <td>4.519258</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.231113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yy7shAsNWRbGg-8Y67Dzag</td>\n",
       "      <td>rS39YnrhoXmPqHLzCBjeqw</td>\n",
       "      <td>2.854908</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.021052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142039</th>\n",
       "      <td>pA9NXgASl86RImkdBtydrA</td>\n",
       "      <td>q6-SF8zHFU1AWO70k92o1Q</td>\n",
       "      <td>3.354326</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.834200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142040</th>\n",
       "      <td>_eUb7UGsUoSfi9n2ieF5ow</td>\n",
       "      <td>hgWMxKhrnOUd3m5nOUBIkA</td>\n",
       "      <td>3.055586</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.891918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142041</th>\n",
       "      <td>cEJGXB63KhROA-XmE_jgXw</td>\n",
       "      <td>0ldxjei8v4q95fApIei3Lg</td>\n",
       "      <td>3.580641</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.014579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142042</th>\n",
       "      <td>Z4-V0hc51oxUdULWJOufeg</td>\n",
       "      <td>j29tuUdrfaxmGjwxHdHZPA</td>\n",
       "      <td>3.958234</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.918212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142043</th>\n",
       "      <td>qUL3CdRRF1vedNvaq06rIA</td>\n",
       "      <td>AYL_y8ahquUW0o-cvIyLbg</td>\n",
       "      <td>3.657034</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.117626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142044 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       user_id             business_id  prediction  stars  \\\n",
       "0       wf1GqnKQuvH-V3QN80UOOQ  fThrN4tfupIGetkrz18JOg    3.870688    5.0   \n",
       "1       39FT2Ui8KUXwmUt6hnwy-g  uW6UHfONAmm8QttPkbMewQ    4.914444    5.0   \n",
       "2       7weuSPSSqYLUFga6IYP4pg  IhNASEZ3XnBHmuuVnWdIwA    4.886283    4.0   \n",
       "3       CqaIzLiWaa-lMFYBAsYQxw  G859H6xfAmVLxbzQgipuoA    4.519258    5.0   \n",
       "4       yy7shAsNWRbGg-8Y67Dzag  rS39YnrhoXmPqHLzCBjeqw    2.854908    3.0   \n",
       "...                        ...                     ...         ...    ...   \n",
       "142039  pA9NXgASl86RImkdBtydrA  q6-SF8zHFU1AWO70k92o1Q    3.354326    2.0   \n",
       "142040  _eUb7UGsUoSfi9n2ieF5ow  hgWMxKhrnOUd3m5nOUBIkA    3.055586    4.0   \n",
       "142041  cEJGXB63KhROA-XmE_jgXw  0ldxjei8v4q95fApIei3Lg    3.580641    5.0   \n",
       "142042  Z4-V0hc51oxUdULWJOufeg  j29tuUdrfaxmGjwxHdHZPA    3.958234    3.0   \n",
       "142043  qUL3CdRRF1vedNvaq06rIA  AYL_y8ahquUW0o-cvIyLbg    3.657034    4.0   \n",
       "\n",
       "            rmse  \n",
       "0       1.275345  \n",
       "1       0.007320  \n",
       "2       0.785498  \n",
       "3       0.231113  \n",
       "4       0.021052  \n",
       "...          ...  \n",
       "142039  1.834200  \n",
       "142040  0.891918  \n",
       "142041  2.014579  \n",
       "142042  0.918212  \n",
       "142043  0.117626  \n",
       "\n",
       "[142044 rows x 5 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>prediction_CF</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wf1GqnKQuvH-V3QN80UOOQ</td>\n",
       "      <td>fThrN4tfupIGetkrz18JOg</td>\n",
       "      <td>4.226697</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39FT2Ui8KUXwmUt6hnwy-g</td>\n",
       "      <td>uW6UHfONAmm8QttPkbMewQ</td>\n",
       "      <td>4.593158</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7weuSPSSqYLUFga6IYP4pg</td>\n",
       "      <td>IhNASEZ3XnBHmuuVnWdIwA</td>\n",
       "      <td>4.002277</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CqaIzLiWaa-lMFYBAsYQxw</td>\n",
       "      <td>G859H6xfAmVLxbzQgipuoA</td>\n",
       "      <td>4.471368</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yy7shAsNWRbGg-8Y67Dzag</td>\n",
       "      <td>rS39YnrhoXmPqHLzCBjeqw</td>\n",
       "      <td>3.660040</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142039</th>\n",
       "      <td>pA9NXgASl86RImkdBtydrA</td>\n",
       "      <td>q6-SF8zHFU1AWO70k92o1Q</td>\n",
       "      <td>3.290036</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142040</th>\n",
       "      <td>_eUb7UGsUoSfi9n2ieF5ow</td>\n",
       "      <td>hgWMxKhrnOUd3m5nOUBIkA</td>\n",
       "      <td>3.375562</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142041</th>\n",
       "      <td>cEJGXB63KhROA-XmE_jgXw</td>\n",
       "      <td>0ldxjei8v4q95fApIei3Lg</td>\n",
       "      <td>3.431802</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142042</th>\n",
       "      <td>Z4-V0hc51oxUdULWJOufeg</td>\n",
       "      <td>j29tuUdrfaxmGjwxHdHZPA</td>\n",
       "      <td>3.731592</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142043</th>\n",
       "      <td>qUL3CdRRF1vedNvaq06rIA</td>\n",
       "      <td>AYL_y8ahquUW0o-cvIyLbg</td>\n",
       "      <td>3.413103</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142044 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       user_id             business_id  prediction_CF  num\n",
       "0       wf1GqnKQuvH-V3QN80UOOQ  fThrN4tfupIGetkrz18JOg       4.226697   34\n",
       "1       39FT2Ui8KUXwmUt6hnwy-g  uW6UHfONAmm8QttPkbMewQ       4.593158   60\n",
       "2       7weuSPSSqYLUFga6IYP4pg  IhNASEZ3XnBHmuuVnWdIwA       4.002277  115\n",
       "3       CqaIzLiWaa-lMFYBAsYQxw  G859H6xfAmVLxbzQgipuoA       4.471368   24\n",
       "4       yy7shAsNWRbGg-8Y67Dzag  rS39YnrhoXmPqHLzCBjeqw       3.660040   67\n",
       "...                        ...                     ...            ...  ...\n",
       "142039  pA9NXgASl86RImkdBtydrA  q6-SF8zHFU1AWO70k92o1Q       3.290036   25\n",
       "142040  _eUb7UGsUoSfi9n2ieF5ow  hgWMxKhrnOUd3m5nOUBIkA       3.375562   29\n",
       "142041  cEJGXB63KhROA-XmE_jgXw  0ldxjei8v4q95fApIei3Lg       3.431802   38\n",
       "142042  Z4-V0hc51oxUdULWJOufeg  j29tuUdrfaxmGjwxHdHZPA       3.731592   22\n",
       "142043  qUL3CdRRF1vedNvaq06rIA  AYL_y8ahquUW0o-cvIyLbg       3.413103  239\n",
       "\n",
       "[142044 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CF_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pandas.to_csv(output_file, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>prediction_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wf1GqnKQuvH-V3QN80UOOQ</td>\n",
       "      <td>fThrN4tfupIGetkrz18JOg</td>\n",
       "      <td>3.928601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39FT2Ui8KUXwmUt6hnwy-g</td>\n",
       "      <td>uW6UHfONAmm8QttPkbMewQ</td>\n",
       "      <td>4.836473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7weuSPSSqYLUFga6IYP4pg</td>\n",
       "      <td>IhNASEZ3XnBHmuuVnWdIwA</td>\n",
       "      <td>4.876303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CqaIzLiWaa-lMFYBAsYQxw</td>\n",
       "      <td>G859H6xfAmVLxbzQgipuoA</td>\n",
       "      <td>4.655522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yy7shAsNWRbGg-8Y67Dzag</td>\n",
       "      <td>rS39YnrhoXmPqHLzCBjeqw</td>\n",
       "      <td>2.926197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142039</th>\n",
       "      <td>pA9NXgASl86RImkdBtydrA</td>\n",
       "      <td>q6-SF8zHFU1AWO70k92o1Q</td>\n",
       "      <td>3.449915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142040</th>\n",
       "      <td>_eUb7UGsUoSfi9n2ieF5ow</td>\n",
       "      <td>hgWMxKhrnOUd3m5nOUBIkA</td>\n",
       "      <td>3.134998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142041</th>\n",
       "      <td>cEJGXB63KhROA-XmE_jgXw</td>\n",
       "      <td>0ldxjei8v4q95fApIei3Lg</td>\n",
       "      <td>3.539382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142042</th>\n",
       "      <td>Z4-V0hc51oxUdULWJOufeg</td>\n",
       "      <td>j29tuUdrfaxmGjwxHdHZPA</td>\n",
       "      <td>3.958909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142043</th>\n",
       "      <td>qUL3CdRRF1vedNvaq06rIA</td>\n",
       "      <td>AYL_y8ahquUW0o-cvIyLbg</td>\n",
       "      <td>3.672221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142044 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       user_id             business_id  prediction_model\n",
       "0       wf1GqnKQuvH-V3QN80UOOQ  fThrN4tfupIGetkrz18JOg          3.928601\n",
       "1       39FT2Ui8KUXwmUt6hnwy-g  uW6UHfONAmm8QttPkbMewQ          4.836473\n",
       "2       7weuSPSSqYLUFga6IYP4pg  IhNASEZ3XnBHmuuVnWdIwA          4.876303\n",
       "3       CqaIzLiWaa-lMFYBAsYQxw  G859H6xfAmVLxbzQgipuoA          4.655522\n",
       "4       yy7shAsNWRbGg-8Y67Dzag  rS39YnrhoXmPqHLzCBjeqw          2.926197\n",
       "...                        ...                     ...               ...\n",
       "142039  pA9NXgASl86RImkdBtydrA  q6-SF8zHFU1AWO70k92o1Q          3.449915\n",
       "142040  _eUb7UGsUoSfi9n2ieF5ow  hgWMxKhrnOUd3m5nOUBIkA          3.134998\n",
       "142041  cEJGXB63KhROA-XmE_jgXw  0ldxjei8v4q95fApIei3Lg          3.539382\n",
       "142042  Z4-V0hc51oxUdULWJOufeg  j29tuUdrfaxmGjwxHdHZPA          3.958909\n",
       "142043  qUL3CdRRF1vedNvaq06rIA  AYL_y8ahquUW0o-cvIyLbg          3.672221\n",
       "\n",
       "[142044 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(alpha=0, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "             objective='reg:linear', random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "             scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wf1GqnKQuvH-V3QN80UOOQ', 'fThrN4tfupIGetkrz18JOg', 3.8706881999969482]\n"
     ]
    }
   ],
   "source": [
    "for row in answer.itertuples():\n",
    "    print(list(row))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration :  5940.175657987595\n"
     ]
    }
   ],
   "source": [
    "    with open(output_file, 'w') as csvfile:\n",
    "        writer_ = csv.writer(csvfile, delimiter=',')\n",
    "        writer_.writerow([\"user_id\", \"business_id\", \"prediction\"])\n",
    "        for line in answer.itertuples():\n",
    "            writer_.writerow([line[0], line[1], line[2]])\n",
    "\n",
    "    print(\"Duration : \", time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
