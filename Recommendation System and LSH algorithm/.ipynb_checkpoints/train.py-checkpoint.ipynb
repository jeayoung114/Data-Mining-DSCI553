{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train test를 나눈다\n",
    "answer- prediction 이 뭐가 더 큰지로 +-1 labeling\n",
    "classifier train\n",
    "\n",
    "model\n",
    "\n",
    "classifier 결과에 따라서 값 달리함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "import json\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "import sys\n",
    "from pyspark import SparkContext \n",
    "from sklearn import preprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext('local[*]', 'task2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/Users/jaeyoungkim/Desktop/usc/DSCI - 553/hw/hw3/\"\n",
    "test_file_name = \"yelp_val_in.csv\"\n",
    "output_file = \"task2_3.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_ftn(user_id, business_id, prediction_CF, prediction_model, num, user_dict, business_dict):\n",
    "    if prediction_CF == -1:\n",
    "        return prediction_model\n",
    "    else:\n",
    "        user_review_count = user_dict[user_id][\"n_review_review\"]\n",
    "        business_review_count = business_dict[business_id][\"n_review_review\"]\n",
    "        if num< 100:\n",
    "            return prediction_model\n",
    "        else:   \n",
    "            model_count = (user_review_count+business_review_count)/2\n",
    "            a = (num/(model_count+num))*0.3\n",
    "            return a * prediction_CF + (1-a) * prediction_model\n",
    "        \n",
    "def len_dict(x):\n",
    "    if x == \"None\":\n",
    "        return 0\n",
    "    elif x == None:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(x)\n",
    "    \n",
    "def date_cal(x):\n",
    "#     print(x)\n",
    "    return (datetime.date(2021,3,10)-datetime.date(int(x.split(\"-\")[0]),int(x.split(\"-\")[1]),int(x.split(\"-\")[2]))).days\n",
    "\n",
    "def get_length(x):\n",
    "    if x ==\"None\":\n",
    "        return 0\n",
    "    elif x == None:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(x.split(\",\"))\n",
    "\n",
    "def get_length_1(x):\n",
    "    if x ==\"None\":\n",
    "        return 0\n",
    "    elif x == None:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(x)\n",
    "    \n",
    "def get_max(x):\n",
    "    if x == \"None\":\n",
    "        return 0\n",
    "    elif x == None:\n",
    "        return 0\n",
    "    else : \n",
    "        return int(sorted(x.split(\",\"), reverse = True)[0])\n",
    "        \n",
    "def avg(x):\n",
    "    a =0\n",
    "    for i in x:\n",
    "        a+=i\n",
    "    return a/len(x)\n",
    "def summation(x):\n",
    "    a =0\n",
    "    for i in x:\n",
    "        a+=i\n",
    "    return a\n",
    "\n",
    "def dict_merge(a,b):\n",
    "    a.update(b)\n",
    "    return a\n",
    "\n",
    "def similarity(business1, business2):\n",
    "    a = business_basket_dict[business1]\n",
    "    b = business_basket_dict[business2]\n",
    "    \n",
    "    if len(a) ==0 or len(b) ==0:\n",
    "        return 0\n",
    "    else:\n",
    "        avg_a = sum(a.values())/len(a)\n",
    "        avg_b = sum(b.values())/len(b)\n",
    "\n",
    "        div_a = sum([(i-avg_a)**2 for i in a.values()])**0.5\n",
    "        div_b = sum([(i-avg_b)**2 for i in b.values()])**0.5\n",
    "\n",
    "        num = 0\n",
    "        for i in set(a.keys()).intersection(set(b.keys())):\n",
    "            num += (a[i]-avg_a)*(b[i]-avg_b)\n",
    "\n",
    "        if div_a ==0 or div_b == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return num/(div_a*div_b)+0.1 ## to remove negative similarities\n",
    "\n",
    "    \n",
    "def score(x, user):  ## neighborhood 2\n",
    "    num = 0\n",
    "    div = 0\n",
    "    x = sorted(x, key=lambda a: -a[0])\n",
    "    count = 0\n",
    "    while count<3:\n",
    "        count+=1\n",
    "        for a,b in x: ## w, r\n",
    "            if a !=0:\n",
    "                num += a*b\n",
    "                div += abs(a)\n",
    "    if div == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return num/div\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-15-85fc6737f678>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-85fc6737f678>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    X_test[\"y_test\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "whole_data = pd.read_csv(folder_path + 'yelp_train.csv')\n",
    "whole_data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(whole_data.loc[:,whole_data.columns!=\"stars\"], whole_data[\"stars\"] , test_size=0.2, random_state=42)\n",
    "X_train[\"stars\"] = y_train\n",
    "X_test[\"y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration :  12.142004013061523\n",
      "Duration :  15.15271806716919\n",
      "Duration :  15.165454149246216\n",
      "Duration :  329.351279258728\n",
      "Duration :  338.486426115036\n",
      "Duration :  348.2463541030884\n",
      "[18:28:09] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[18:28:31] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Xgb train Duration :  379.8590021133423\n",
      "Duration :  379.8595058917999\n",
      "Duration :  390.36818408966064\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "   \n",
    "#     input_file = sys.argv[1]\n",
    "#     output_file = sys.argv[2]\n",
    "    \n",
    "    t = time.time()\n",
    "    n = 30\n",
    "    \n",
    "\n",
    "    \n",
    "    ## RDDs\n",
    "    textRDD = sc.textFile(folder_path + 'yelp_train.csv', n)\n",
    "    testRDD = sc.textFile(test_file_name, n)\n",
    "    \n",
    "    ## exclue header\n",
    "    header = textRDD.first()  ## remove header\n",
    "    header2 = testRDD.first()  ## remove header\n",
    "    tmp_train = textRDD.filter(lambda x: x != header)\n",
    "    tmp_test = testRDD.filter(lambda x: x!= header2)\n",
    "    \n",
    "    \n",
    "    ##make dictionary for user and business\n",
    "    user_list_train = tmp_train.map(lambda x : (x.split(\",\")[0],1)).reduceByKey(lambda a,b : a).map(lambda x : (1,[x[0]])).reduceByKey(lambda a,b : a+b).collect()[0][1]\n",
    "    user_list_test = tmp_test.map(lambda x : (x.split(\",\")[0],1)).reduceByKey(lambda a,b : a).map(lambda x : (1,[x[0]])).reduceByKey(lambda a,b : a+b).collect()[0][1]\n",
    "    user_list = list(set(user_list_train+user_list_test))\n",
    "    user_to_idx = {}\n",
    "    for idx, user in enumerate(user_list):\n",
    "        user_to_idx[user] = idx\n",
    "        \n",
    "    business_list_train = tmp_train.map(lambda x : (x.split(\",\")[1],1)).reduceByKey(lambda a,b : a).map(lambda x : (1,[x[0]])).reduceByKey(lambda a,b : a+b).collect()[0][1]\n",
    "    business_list_test = tmp_test.map(lambda x : (x.split(\",\")[1],1)).reduceByKey(lambda a,b : a).map(lambda x : (1,[x[0]])).reduceByKey(lambda a,b : a+b).collect()[0][1]\n",
    "    business_list = list(set(business_list_train+business_list_test))\n",
    "    business_to_idx = {}\n",
    "    for idx, business in enumerate(business_list):\n",
    "        business_to_idx[business] = idx\n",
    "        \n",
    "    ###################################\n",
    "    ##### Collaborative Filtering #####\n",
    "    ###################################\n",
    "        \n",
    "    ## user basket\n",
    "    \n",
    "    user_basket_dict = tmp_train.map(lambda x : (user_to_idx[x.split(\",\")[0]], {business_to_idx[x.split(\",\")[1]]: float(x.split(\",\")[2])}))\\\n",
    "    .reduceByKey(lambda a,b : dict_merge(a,b)).map(lambda x : (1,{x[0]:x[1]})).reduceByKey(lambda a,b : dict_merge(a,b)).map(lambda x : x[1]).collect()[0]\n",
    "    \n",
    "    for user_id in user_to_idx.values():\n",
    "        if user_id not in user_basket_dict.keys():\n",
    "            user_basket_dict[user_id] = {}\n",
    "        \n",
    "    print(\"Duration : \", time.time()-t)\n",
    "\n",
    "    ## business basket\n",
    "    \n",
    "    business_basket_dict = tmp_train.map(lambda x : (business_to_idx[x.split(\",\")[1]], {user_to_idx[x.split(\",\")[0]]: float(x.split(\",\")[2])}))\\\n",
    "    .reduceByKey(lambda a,b : dict_merge(a,b)).map(lambda x : (1,{x[0]:x[1]})).reduceByKey(lambda a,b : dict_merge(a,b)).map(lambda x : x[1]).collect()[0]\n",
    "    print(\"Duration : \", time.time()-t)\n",
    "    \n",
    "    for business_id in business_to_idx.values():\n",
    "        if business_id not in business_basket_dict.keys():\n",
    "            business_basket_dict[business_id] = {}\n",
    "            \n",
    "    ## id to user, id to business\n",
    "    \n",
    "    idx_to_user = {v: k for k, v in user_to_idx.items()}\n",
    "    idx_to_business = {v: k for k, v in business_to_idx.items()}\n",
    "    \n",
    "    print(\"Duration : \", time.time()-t)\n",
    "    \n",
    "    \n",
    "    val = tmp_test.map(lambda x : (user_to_idx[x.split(\",\")[0]], business_to_idx[x.split(\",\")[1]]))\\\n",
    "        .map(lambda x : (x[0],x[1], user_basket_dict[x[0]]))\\\n",
    "        .map(lambda x : (x[0],x[1],[(similarity(x[1], k), v) for k,v in x[2].items()], get_length_1(x[2])))\\\n",
    "        .map(lambda x : (idx_to_user[x[0]],idx_to_business[x[1]],score(x[2], x[0]), x[3]))\n",
    "     \n",
    "    answer_CF = val.collect()\n",
    "\n",
    "    #################################\n",
    "    ##### Model Based Filtering #####\n",
    "    #################################\n",
    "\n",
    "    ##create user_dictionary, business_dictionary\n",
    "        \n",
    "    business_dict = {}\n",
    "    user_dict = {}\n",
    "    \n",
    "    for user in user_list:\n",
    "        user_dict[user] = {}\n",
    "    for business in business_list:\n",
    "        business_dict[business] = {}\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    ### fill user_dict and business_dict with json files\n",
    "    '''\n",
    "    checkin : checkin sum, avg for business\n",
    "\n",
    "    business : stars, review_count, latitude, longitude, how many days does it open (len(hours)), attribute length?, \n",
    "\n",
    "    photo  : number of photos\n",
    "\n",
    "    review : number of reviews based on user, number of reviews based on business, avg star, useful (sum avg), funny (sum, avg), cool (sum, avg)\n",
    "\n",
    "    tip : number of tips based on user, number of tips based on business, number of likes\n",
    "\n",
    "    user : review_count, daycount of (today -yelpsince), number of friend , useful, funny, cool, fans, most reent \"elite\" year, number of \"elite\", avgstars, \n",
    "\n",
    "    compliment_hot, more, profile, cute, list, note plain, cool, funny, writer, photos\n",
    "    \n",
    "    '''\n",
    "    ### Extract Features and then merge it into user_dict and busines_dict\n",
    "    \n",
    "    ## checkin\n",
    "    checkinRDD = sc.textFile(folder_path + \"checkin.json\", n)\n",
    "    checkin = checkinRDD.map(lambda line : json.loads(line)).map(lambda x : (x[\"business_id\"], x[\"time\"])).map(lambda x: (x[0], list(x[1].values()))).map(lambda x : (x[0], summation(x[1]),avg(x[1]))).collect()\n",
    "    for i in checkin:\n",
    "        try:\n",
    "            business_dict[i[0]][\"checkin_sum\"] = i[1]\n",
    "            business_dict[i[0]][\"checkin_avg\"] = i[2]\n",
    "        except:\n",
    "            continue\n",
    "    del checkin\n",
    "    \n",
    "    # business\n",
    "    businessRDD = sc.textFile(folder_path + \"business.json\", n)\n",
    "    business = businessRDD.map(lambda line : json.loads(line))\\\n",
    "    .map(lambda x : (x[\"business_id\"], x[\"latitude\"], x[\"longitude\"], x[\"stars\"], x[\"review_count\"], x[\"is_open\"], len_dict(x[\"attributes\"]), get_length(x[\"categories\"]), len_dict(x[\"hours\"]))).collect()\n",
    "\n",
    "\n",
    "    for i in business:\n",
    "        try:\n",
    "            business_dict[i[0]][\"latitude\"] = i[1]\n",
    "            business_dict[i[0]][\"longitude\"] = i[2]\n",
    "            business_dict[i[0]][\"stars\"] = i[3]\n",
    "            business_dict[i[0]][\"review_count\"] = i[4]\n",
    "            business_dict[i[0]][\"is_open\"] = i[5]\n",
    "            business_dict[i[0]][\"len_attributes\"] = i[6]\n",
    "            business_dict[i[0]][\"len_categories\"] = i[7]\n",
    "            business_dict[i[0]][\"len_hours\"] = i[8]\n",
    "        except:\n",
    "            continue\n",
    "    del business\n",
    "            \n",
    "    \n",
    "    # photo\n",
    "    photoRDD = sc.textFile(folder_path + \"photo.json\", n)\n",
    "    photo = photoRDD.map(lambda line : json.loads(line))\\\n",
    "            .map(lambda x: (x[\"business_id\"], 1)).reduceByKey(lambda a,b : a+b).collect()\n",
    "    # .map(lambda x : (x[\"business_id\"], x[\"latitude\"], x[\"longitude\"], x[\"stars\"], x[\"review_count\"], x[\"is_open\"], len_dict(x[\"attributes\"]), get_length(x[\"categories\"]), len_dict(x[\"hours\"]))).collect()\n",
    "\n",
    "\n",
    "    for i in photo:\n",
    "        try:\n",
    "            business_dict[i[0]][\"n_photo\"] = i[1]\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    del photo\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # review\n",
    "    reviewRDD = sc.textFile(folder_path + \"review_train.json\", n)\n",
    "    review = reviewRDD.map(lambda line : json.loads(line))\\\n",
    "            .map(lambda x: (x[\"business_id\"], (1, x[\"stars\"], len(x[\"text\"]) ))).reduceByKey(lambda a,b : (a[0]+b[0], a[1]+b[1], a[2]+b[2]))\\\n",
    "    .map(lambda x: (x[0], x[1][0], x[1][1]/x[1][0], x[1][2]/x[1][0])).collect()\n",
    "\n",
    "    for i in review:\n",
    "        try:\n",
    "            business_dict[i[0]][\"n_review_review\"] = i[1]\n",
    "            business_dict[i[0]][\"avg_review_stars\"] = i[2]\n",
    "            business_dict[i[0]][\"avg_review_len\"] = i[3]\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "    review = reviewRDD.map(lambda line : json.loads(line))\\\n",
    "            .map(lambda x: (x[\"user_id\"], (1, x[\"stars\"], len(x[\"text\"]) ))).reduceByKey(lambda a,b : (a[0]+b[0], a[1]+b[1], a[2]+b[2]))\\\n",
    "    .map(lambda x: (x[0], x[1][0], x[1][1]/x[1][0], x[1][2]/x[1][0])).collect()\n",
    "\n",
    "    for i in review:\n",
    "        try:\n",
    "            user_dict[i[0]][\"n_review_review\"] = i[1]\n",
    "            user_dict[i[0]][\"avg_review_stars\"] = i[2]\n",
    "            user_dict[i[0]][\"avg_review_len\"] = i[3]\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    del review\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # tip\n",
    "    tipRDD = sc.textFile(folder_path + \"tip.json\", n)\n",
    "    tip = tipRDD.map(lambda line : json.loads(line))\\\n",
    "            .map(lambda x: (x[\"business_id\"], (1, x[\"likes\"], len(x[\"text\"]) ))).reduceByKey(lambda a,b : (a[0]+b[0], a[1]+b[1], a[2]+b[2]))\\\n",
    "            .map(lambda x: (x[0], x[1][0], x[1][1]/x[1][0], x[1][2]/x[1][0])).collect()\n",
    "\n",
    "\n",
    "    for i in tip:\n",
    "        try:\n",
    "            business_dict[i[0]][\"n_tip_business\"] = i[1]\n",
    "            business_dict[i[0]][\"avg_like_business\"] = i[2]\n",
    "            business_dict[i[0]][\"avg_tip_len_business\"] = i[3]\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    tip = tipRDD.map(lambda line : json.loads(line))\\\n",
    "            .map(lambda x: (x[\"user_id\"], (1, x[\"likes\"], len(x[\"text\"]) ))).reduceByKey(lambda a,b : (a[0]+b[0], a[1]+b[1], a[2]+b[2]))\\\n",
    "            .map(lambda x: (x[0], x[1][0], x[1][1]/x[1][0], x[1][2]/x[1][0])).collect()\n",
    "\n",
    "\n",
    "    for i in tip:\n",
    "        try:\n",
    "            user_dict[i[0]][\"n_tip_user\"] = i[1]\n",
    "            user_dict[i[0]][\"avg_like_user\"] = i[2]\n",
    "            user_dict[i[0]][\"avg_tip_len_user\"] = i[3]\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    del tip\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## user\n",
    "    userRDD = sc.textFile(folder_path + \"user.json\", n).\\\n",
    "        map(lambda line : json.loads(line))\\\n",
    "        .filter(lambda x : x[\"user_id\"] in user_list)\n",
    "    user_data = userRDD.map(lambda x : (x[\"user_id\"], x[\"review_count\"],\\\n",
    "                     (datetime.date(2021,3,10)-datetime.date(int(x[\"yelping_since\"].split(\"-\")[0]),int(x[\"yelping_since\"].split(\"-\")[1]),int(x[\"yelping_since\"].split(\"-\")[2]))).days,\\\n",
    "                     get_length(x[\"friends\"]),\\\n",
    "                     x[\"useful\"],\\\n",
    "                    x[\"funny\"],\\\n",
    "                    x[\"fans\"],\\\n",
    "                    get_length(x[\"elite\"]),\\\n",
    "                    get_max(x[\"elite\"]),\\\n",
    "                    x[\"average_stars\"],\\\n",
    "                    x[\"compliment_hot\"],\\\n",
    "                    x[\"compliment_more\"],\\\n",
    "                    x[\"compliment_cute\"],\\\n",
    "                     x[\"compliment_list\"],\\\n",
    "                     x[\"compliment_note\"],\\\n",
    "                     x[\"compliment_plain\"],\\\n",
    "                     x[\"compliment_cool\"],\\\n",
    "                     x[\"compliment_funny\"],\\\n",
    "                     x[\"compliment_writer\"],\\\n",
    "                     x[\"compliment_photos\"],\\\n",
    "                    )).collect()\n",
    "\n",
    "    \n",
    "    \n",
    "    for i in user_data:\n",
    "        try:\n",
    "            user_dict[i[0]][\"review_count\"] = i[1]\n",
    "            user_dict[i[0]][\"date_since\"] = i[2]\n",
    "            user_dict[i[0]][\"n_friends\"] = i[3]\n",
    "            user_dict[i[0]][\"useful\"] = i[4]\n",
    "            user_dict[i[0]][\"funny\"] = i[5]\n",
    "            user_dict[i[0]][\"fans\"] = i[6]\n",
    "            user_dict[i[0]][\"n_elite\"] = i[7]\n",
    "            user_dict[i[0]][\"max_elite\"] = i[8]\n",
    "            user_dict[i[0]][\"avg_stars\"] = i[9]\n",
    "            user_dict[i[0]][\"compliment_hot\"] = i[10]\n",
    "            user_dict[i[0]][\"compliment_more\"] = i[11]\n",
    "            user_dict[i[0]][\"compliment_cute\"] = i[12]\n",
    "            user_dict[i[0]][\"compliment_list\"] = i[13]\n",
    "            user_dict[i[0]][\"compliment_note\"] = i[14]\n",
    "            user_dict[i[0]][\"compliment_plain\"] = i[15]\n",
    "            user_dict[i[0]][\"compliment_cool\"] = i[16]\n",
    "            user_dict[i[0]][\"compliment_funny\"] = i[17]\n",
    "            user_dict[i[0]][\"compliment_writer\"] = i[18]\n",
    "            user_dict[i[0]][\"compliment_photos\"] = i[19]\n",
    "            \n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "    del user_data\n",
    "            \n",
    "    print(\"Duration : \", time.time()-t)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## make train_data from business_dict and user_dict\n",
    "    ## \n",
    "    \n",
    "    train_vec = tmp_train.map(lambda x: (x.split(\",\")[0],x.split(\",\")[1], x.split(\",\")[2]))\\\n",
    "        .map(lambda x : (x[0], x[1], user_dict[x[0]],business_dict[x[1]], float(x[2])))\\\n",
    "    #         .map(lambda x : (x[0], x[1], x[2].update(x[3])))\n",
    "    train_vec = train_vec.collect()\n",
    "    print(\"Duration : \", time.time()-t)\n",
    "    \n",
    "\n",
    "    train_data = pd.DataFrame()\n",
    "    data_row = []\n",
    "    count =0\n",
    "    for train_row in train_vec:\n",
    "        count+=1\n",
    "        row = {}\n",
    "        for i in train_row[2].items():\n",
    "            row[i[0]] = i[1]\n",
    "        for i in train_row[3].items():\n",
    "            row[i[0]] = i[1]\n",
    "        row[\"target\"] = train_row[4]\n",
    "        data_row.append(row)\n",
    "\n",
    "    train_data = pd.DataFrame.from_dict(data_row)\n",
    "    print(\"Duration : \", time.time()-t)\n",
    "\n",
    "    \n",
    "    ## minmax scaler\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    train_data_no_label = train_data.loc[:,train_data.columns != 'target']\n",
    "    train_data_no_label_scaled = pd.DataFrame(scaler.fit_transform(train_data_no_label), index = train_data_no_label.index, columns = train_data_no_label.columns)\n",
    "    train_data_no_label_scaled[\"target\"] =  train_data[\"target\"]\n",
    "    train_data  = train_data_no_label_scaled\n",
    "    train_data.fillna(-1)\n",
    "    \n",
    "    \n",
    "    ## make test vector\n",
    "    test_vec = tmp_test.map(lambda x: (x.split(\",\")[0],x.split(\",\")[1]))\\\n",
    "            .map(lambda x : (x[0], x[1], user_dict[x[0]],business_dict[x[1]]))\n",
    "    test_vec = test_vec.collect()## user , business\n",
    "    \n",
    "    \n",
    "    test_data = pd.DataFrame()\n",
    "    data_row = []\n",
    "    count =0\n",
    "    for test_row in test_vec:\n",
    "        count+=1\n",
    "        row = {}\n",
    "        for i in test_row[2].items():\n",
    "            row[i[0]] = i[1]\n",
    "        for i in test_row[3].items():\n",
    "            row[i[0]] = i[1]\n",
    "#         row[\"target\"] = train_row[4]\n",
    "        data_row.append(row)\n",
    "\n",
    "    test_data = pd.DataFrame.from_dict(data_row)\n",
    "    test_data = test_data.fillna(-1)\n",
    "    \n",
    "    ## minmax scaler\n",
    "    test_data = pd.DataFrame(scaler.transform(test_data), index=test_data.index, columns=test_data.columns)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## train xgboost regressor\n",
    "    xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 10, alpha = 0, n_estimators = 100, random_state = 0)\n",
    "    \n",
    "    selected_columns = ['stars',\n",
    "  'avg_review_stars',\n",
    "  'avg_stars',\n",
    "  'len_hours',\n",
    "  'is_open',\n",
    "  'n_elite',\n",
    "  'compliment_photos',\n",
    "  'compliment_hot',\n",
    "  'compliment_funny',\n",
    "  'n_friends',\n",
    "  'max_elite',\n",
    "  'latitude']\n",
    "    xg_reg.fit(train_data.loc[:, train_data.columns != 'target'][selected_columns],train_data[\"target\"])\n",
    "    print(\"Xgb train Duration : \", time.time() - t)\n",
    "    \n",
    "\n",
    "    \n",
    "    print(\"Duration : \", time.time()-t)\n",
    "    \n",
    "    predictions = xg_reg.predict(test_data[selected_columns])\n",
    "    \n",
    "\n",
    "    answer_model = []\n",
    "    for test , score in zip(test_vec, predictions):\n",
    "        answer_model.append((test[0], test[1], score))\n",
    "        \n",
    "        \n",
    "    CF_pandas = pd.DataFrame(answer_CF, columns = [\"user_id\", \"business_id\", \"prediction_CF\", \"num\"])\n",
    "    model_pandas = pd.DataFrame(answer_model, columns = [\"user_id\", \"business_id\", \"prediction_model\"])\n",
    "    merged_df = pd.merge(CF_pandas, model_pandas,  how='left', left_on=['user_id','business_id'], right_on = ['user_id','business_id'])\n",
    "    merged_df[\"prediction\"] = merged_df.apply(lambda x : prediction_ftn(x[\"user_id\"], x[\"business_id\"], x[\"prediction_CF\"], x[\"prediction_model\"], x[\"num\"], user_dict, business_dict), axis = 1)\n",
    "    answer = merged_df[[\"user_id\", \"business_id\", \"prediction\"]]\n",
    "    answer = answer.set_index(\"user_id\")\n",
    "    answer.to_csv(output_file, sep = ',')\n",
    "    \n",
    "\n",
    "    print(\"Duration : \", time.time() - t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :  0.9882530124012295\n"
     ]
    }
   ],
   "source": [
    "res = pd.read_csv(output_file)\n",
    "val = pd.read_csv(\"yelp_val.csv\")\n",
    "\n",
    "new_df = pd.merge(res, val,  how='left', left_on=['user_id','business_id'], right_on = ['user_id','business_id'])\n",
    "\n",
    "new_df[\"rmse\"] = new_df[\"prediction\"]-new_df[\"stars\"]\n",
    "\n",
    "new_df[\"rmse\"] = new_df.rmse.apply(lambda x : x**2)\n",
    "\n",
    "print(\"RMSE : \",(sum(new_df.rmse)/142043)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
