{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\n\tat sun.nio.ch.Net.bind0(Native Method)\n\tat sun.nio.ch.Net.bind(Net.java:461)\n\tat sun.nio.ch.Net.bind(Net.java:453)\n\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:222)\n\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:128)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1283)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501)\n\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486)\n\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:989)\n\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254)\n\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:364)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-359fb55d2137>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'local[*]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'task1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m--> 136\u001b[0;31m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \"\"\"\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1523\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1525\u001b[0;31m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[1;32m   1526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\n\tat sun.nio.ch.Net.bind0(Native Method)\n\tat sun.nio.ch.Net.bind(Net.java:461)\n\tat sun.nio.ch.Net.bind(Net.java:453)\n\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:222)\n\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:128)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1283)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501)\n\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486)\n\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:989)\n\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254)\n\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:364)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "from itertools import combinations\n",
    "\n",
    "from pyspark import SparkContext\n",
    "import os\n",
    "\n",
    "sc = SparkContext('local[*]', 'task1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_function(combination):\n",
    "    result = sum(map(lambda x: int(x), list(combination)))\n",
    "    return result % BUCKET_NUMBER\n",
    "\n",
    "\n",
    "def scaled_down_support_threshold(basket_list, support, entire_size):\n",
    "    \n",
    "    '''\n",
    "    calculate ps, scaled down support threshold\n",
    "    '''\n",
    "    partition_size = len(basket_list)\n",
    "    return math.ceil((partition_size / entire_size) * int(support))\n",
    "\n",
    "def count_singleton(baskets):\n",
    "    '''\n",
    "    generate dictionaray of singleton count\n",
    "    '''\n",
    "    count_num_dict = {}\n",
    "    for basket in baskets:\n",
    "        for item in basket:\n",
    "            count_num_dict[item] = count_num_dict.get(item, 0) + 1\n",
    "    return count_num_dict\n",
    "\n",
    "def get_candidate_singleton(count_dict, ps):\n",
    "    '''\n",
    "    Generate candidate from count dictionary\n",
    "    '''\n",
    "    candidate_list = []\n",
    "    for i in count_dict.keys():\n",
    "        if count_dict[i] >= ps:\n",
    "            candidate_list.append(i)\n",
    "\n",
    "    return candidate_list\n",
    "\n",
    "\n",
    "def get_candidate_pair(count_dict, ps):\n",
    "    '''\n",
    "    Generate candidate from count dictionary\n",
    "    '''\n",
    "    candidate_list = []\n",
    "    for pair in count_dict.keys():\n",
    "        if count_dict[pair] >= ps:\n",
    "            candidate_list.append(pair)\n",
    "\n",
    "    return candidate_list\n",
    "\n",
    "def merge_basket_2d(baskets):\n",
    "    '''\n",
    "    partition to basket list\n",
    "    '''\n",
    "    a = list(baskets)\n",
    "    collected_basket = []\n",
    "    for i in a:\n",
    "        collected_basket += [i]\n",
    "    return collected_basket\n",
    "\n",
    "\n",
    "def generate_permutations(candidate_list, pair_size):\n",
    "    permu_list = []\n",
    "    if len(candidate_list) > 0:\n",
    "        for idx, itemset in enumerate(candidate_list[:-1]):\n",
    "            for itemset2 in candidate_list[idx + 1:]:\n",
    "                if list(itemset[:-1]) == list(itemset2[:-1]):\n",
    "                    candi = sorted(list(itemset) + [itemset2[-1]])\n",
    "                    if candi not in permu_list:\n",
    "                        check_list = [tuple(sorted(i)) for i in list(combinations(candi, pair_size - 1))]\n",
    "                        if set(check_list).issubset(candidate_list):\n",
    "                            permu_list.append(tuple(candi))\n",
    "\n",
    "    return permu_list\n",
    "\n",
    "\n",
    "# def count_pairs(baskets, singleton_candidate):\n",
    "#     count_dict = {}\n",
    "#     for basket in baskets:\n",
    "#         for b1 in basket:\n",
    "#             for b2 in basket:\n",
    "#                 if b1 < b2 and b1 in singleton_candidate and b2 in singleton_candidate:\n",
    "#                             count_dict[(b1, b2)] = count_dict.get((b1, b2), 0) + 1\n",
    "\n",
    "#     return count_dict\n",
    "\n",
    "def get_pair_candidate(baskets, singleton_candidate, ps):\n",
    "    pair_candi_list = []\n",
    "    pair_count = count_pairs(baskets, singleton_candidate)\n",
    "    for candidate in pair_count.keys():\n",
    "        if pair_count[candidate] >= ps:\n",
    "            pair_candi_list.append(candidate)\n",
    "    return pair_candi_list\n",
    "\n",
    "\n",
    "def generate_pairs(singleton_candidate):\n",
    "    pair_list = []\n",
    "    for i in singleton_candidate:\n",
    "        for j in singleton_candidate:\n",
    "            if i<j:\n",
    "                pair_list.append((i,j))\n",
    "    return pair_list\n",
    "\n",
    "def pair_count(baskets, pair_list):\n",
    "    count_dict = {}\n",
    "    for pair in pair_list:\n",
    "        for basket in baskets:\n",
    "            if set(pair).issubset(set(basket)):\n",
    "                count_dict[pair] = count_dict.get(pair, 0) + 1\n",
    "    return count_dict\n",
    "\n",
    "# def get_candidate_pair(count_dict ,ps):\n",
    "#     candidate_list = []\n",
    "#     for pair in count_dict.keys():\n",
    "#         if count_dict[pair]>=ps:\n",
    "#             candidate_list.append(pair)\n",
    "            \n",
    "#     return candidate_list\n",
    "\n",
    "def get_candidate(count_dict, ps):\n",
    "    candidate_list = []\n",
    "    for i in count_dict.keys():\n",
    "        if count_dict[i] >= ps:\n",
    "            candidate_list.append(i)\n",
    "\n",
    "    return candidate_list\n",
    "\n",
    "\n",
    "def apriori(partition, support, entire_size):\n",
    "    '''\n",
    "    perform pariori algorithm in the partition\n",
    "    with scaled down support threshold\n",
    "    '''\n",
    "    ## partition to baskets\n",
    "    baskets = merge_basket_2d(partition)\n",
    "    \n",
    "    \n",
    "    total_candidate = []\n",
    "    \n",
    "    ## singleton\n",
    "    ps = scaled_down_support_threshold(baskets, support, entire_size)\n",
    "    singleton_candidate = sorted(get_candidate(count_singleton(baskets), ps))\n",
    "    total_candidate += [(i,) for i in singleton_candidate]\n",
    "    \n",
    "    ## pairs\n",
    "    pair_size = 2\n",
    "    pairs = generate_pairs(singleton_candidate)\n",
    "    counted_dict = pair_count(baskets, pairs)\n",
    "    pair_candidate = sorted(get_candidate_pair(counted_dict, ps))\n",
    "#     candidate_list = pair_candidate\n",
    "    total_candidate+= pair_candidate\n",
    "    \n",
    "    ##above triples\n",
    "    while len(pair_candidate) > 0:\n",
    "        pair_size += 1  ## pair_size = 3\n",
    "        count_dict = {}\n",
    "        pairs_to_be_counted = generate_permutations(pair_candidate, pair_size)\n",
    "        counted_dict = pair_count(baskets, pairs_to_be_counted)\n",
    "        pair_candidate = sorted(get_candidate_pair(counted_dict, ps))\n",
    "        total_candidate += pair_candidate\n",
    "    \n",
    "#     singleton\n",
    "    \n",
    "    return total_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15 # partition_num\n",
    "input_file = \"small1.csv\"\n",
    "output_file = \"task1.txt\"\n",
    "case = 1\n",
    "s = 4\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    textRDD = sc.textFile(input_file, n)\n",
    "    header = textRDD.first()  ## remove header\n",
    "    tmp = textRDD.filter(lambda x: x != header)\n",
    "    \n",
    "    \n",
    "    if case == 1:\n",
    "        baskets = tmp.map(lambda x: x.split(\",\")).mapValues(lambda x: [x]).reduceByKey(lambda a, b: a + b)\\\n",
    "        .mapValues(lambda x: sorted(list(set(x)))).map(lambda x: x[1])\n",
    "        basket_list = baskets.collect()\n",
    "        \n",
    "    elif case == 2 :\n",
    "        baskets = tmp.map(lambda x: [x.split(\",\")[1],x.split(\",\")[0]]).mapValues(lambda x: [x]).reduceByKey(lambda a, b: a + b)\\\n",
    "        .mapValues(lambda x: sorted(list(set(x)))).map(lambda x: x[1])\n",
    "        basket_list = baskets.collect()\n",
    "        \n",
    "    entire_size = len(basket_list)\n",
    "        \n",
    "    num = baskets.mapPartitions(lambda x: [sum(1 for i in x)]).collect() ## number of items in each partition\n",
    "       \n",
    "    ## Phase 1\n",
    "    \n",
    "    intermediate = baskets.mapPartitions(lambda x : apriori(x, s, entire_size))\\\n",
    "        .map(lambda x: (x, 1)).reduceByKey(lambda a, b: a)\n",
    "    \n",
    "    candidate_itemsets = intermediate.collect()\n",
    "\n",
    "    num = tmp.mapPartitions(lambda x: [sum(1 for i in x)]).collect()  ## number of items in each partition\n",
    "    candidate_itemsets = sorted(candidate_itemsets, key=lambda x: (len(x[0]), x[0]))\n",
    "    \n",
    "    \n",
    "    ## Phase 2\n",
    "\n",
    "    def Map2(basket, candidate_itemsets):\n",
    "        answer_list = []\n",
    "        for candidate in candidate_itemsets:\n",
    "            if set(candidate[0]).issubset(set(basket)):\n",
    "                answer_list.append((candidate[0], 1))\n",
    "        size = len(answer_list)\n",
    "\n",
    "        return answer_list\n",
    "\n",
    "\n",
    "    freq_itemsets = baskets.map(lambda x: Map2(x, candidate_itemsets)).flatMap(lambda x: x).reduceByKey(\n",
    "        lambda a, b: a + b).filter(lambda x: x[1] >= int(s)).map(lambda x: x[0]).collect()\n",
    "\n",
    "    \n",
    "    candidate_itemsets = [candidate[0] for candidate in candidate_itemsets]\n",
    "    freq_itemsets = sorted(freq_itemsets, key=lambda x: (len(x), x))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## file write\n",
    "    \n",
    "    file = open(output_file,\"w\") \n",
    "    file.write(\"Candidates:\")\n",
    "    \n",
    "    \n",
    "    ### format candidate\n",
    "    max_len = len(candidate_itemsets[-1])\n",
    "    answer_list = [\"\" for i in range(max_len+1)]\n",
    "    for candidate in candidate_itemsets:\n",
    "        for i in range(1,max_len+1):\n",
    "            if len(candidate) == i:\n",
    "                answer = \"('\"\n",
    "                for j in range(len(candidate)):\n",
    "                    answer += candidate[j]\n",
    "                    answer += \",\"\n",
    "                answer = answer[:-1]+\"')\"\n",
    "                answer_list[i] += answer\n",
    "                answer_list[i] += ','\n",
    "    for i in range(len(answer_list)):\n",
    "        answer_list[i] = answer_list[i][:-1]\n",
    "        file.write(answer_list[i] + '\\n')\n",
    "    \n",
    "    file.write(\"Frequent itemsets:\")\n",
    "    \n",
    "    ### format frequent itemsets\n",
    "    max_len = len(freq_itemsets[-1])\n",
    "    answer_list = [\"\" for i in range(max_len+1)]\n",
    "    for candidate in freq_itemsets:\n",
    "        for i in range(1,max_len+1):\n",
    "            if len(candidate) == i:\n",
    "                answer = \"('\"\n",
    "                for j in range(len(candidate)):\n",
    "                    answer += candidate[j]\n",
    "                    answer += \",\"\n",
    "                answer = answer[:-1]+\"')\"\n",
    "                answer_list[i] += answer\n",
    "                answer_list[i] += ','\n",
    "    for i in range(len(answer_list)):\n",
    "        answer_list[i] = answer_list[i][:-1]\n",
    "        file.write(answer_list[i] + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singleton_count_bitmap(baskets, ps):\n",
    "    '''\n",
    "    count singletons from basket and create bitmap\n",
    "    '''\n",
    "    \n",
    "    bucket_dict = {}\n",
    "    count_num_dict = {}\n",
    "    bitmap = [0 for i in range(n_bucket)]\n",
    "    # singleton\n",
    "    for basket in baskets:\n",
    "        for item in basket:\n",
    "            count_num_dict[item] = count_num_dict.get(item, 0) + 1\n",
    "    \n",
    "    # bucket\n",
    "        for pair in combinations(basket, 2):\n",
    "            hash_val = hash_function(pair)\n",
    "    #         bucket_dict[hash_val] = bucket_dict.get(hash_val, 0) + 1\n",
    "            bitmap[hash_val] = bitmap[hash_val] + 1\n",
    "        \n",
    "    freq_singleton = sorted(filter_dict(count_num_dict ,ps))\n",
    "    bitmap = count_to_bitmap(bitmap, ps)\n",
    "    \n",
    "    return freq_singleton, bitmap\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "def hash_function(pair):\n",
    "    summation = sum(int(i) for i in pair)\n",
    "    hash_val = summation%n_bucket\n",
    "    return hash_val\n",
    "\n",
    "def filter_dict(dictionary, ps):\n",
    "    return_list = []\n",
    "    for key in dictionary.keys():\n",
    "        if dictionary[key]>=ps:\n",
    "            return_list.append(key)\n",
    "            \n",
    "    return return_list\n",
    "\n",
    "def count_to_bitmap(bitmap, ps):\n",
    "    for i in range(len(bitmap)):\n",
    "        if bitmap[i]>=ps:\n",
    "            bitmap[i] = True\n",
    "        else:\n",
    "            bitmap[i] = False\n",
    "    return bitmap\n",
    "\n",
    "def merge_basket_2d(baskets):\n",
    "    '''\n",
    "    partition to basket list\n",
    "    '''\n",
    "    a = list(baskets)\n",
    "    collected_basket = []\n",
    "    for i in a:\n",
    "        collected_basket += [i]\n",
    "    return collected_basket\n",
    "\n",
    "\n",
    "def scaled_down_support_threshold(basket_list, support, entire_size):\n",
    "    \n",
    "    '''\n",
    "    calculate ps, scaled down support threshold\n",
    "    '''\n",
    "    partition_size = len(basket_list)\n",
    "    return math.ceil((partition_size / entire_size) * int(support))\n",
    "\n",
    "def generate_permutations(candidate_list, pair_size):\n",
    "    permu_list = []\n",
    "    if len(candidate_list) > 0:\n",
    "        for idx, itemset in enumerate(candidate_list[:-1]):\n",
    "            for itemset2 in candidate_list[idx + 1:]:\n",
    "                if list(itemset[:-1]) == list(itemset2[:-1]):\n",
    "                    candi = sorted(list(itemset) + [itemset2[-1]])\n",
    "                    if candi not in permu_list:\n",
    "                        check_list = [tuple(sorted(i)) for i in list(combinations(candi, pair_size - 1))]\n",
    "                        if set(check_list).issubset(candidate_list):\n",
    "                            permu_list.append(tuple(candi))\n",
    "\n",
    "    return permu_list\n",
    "\n",
    "\n",
    "def pair_counter(baskets, ps, freq_singleton, bitmap):\n",
    "    pair_count_dict = {}\n",
    "    for basket in baskets:\n",
    "        for pair in combinations(basket,2):\n",
    "            if pair[0] in freq_singleton and pair[1] in freq_singleton:\n",
    "                if bitmap[hash_function(pair)] == True:\n",
    "                    pair_count_dict[pair] = pair_count_dict.get(pair,0) + 1\n",
    "    return pair_count_dict\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dict(dictionary, ps):\n",
    "    return_list = []\n",
    "    for key in dictionary.keys():\n",
    "        if dictionary[key]>=ps:\n",
    "            return_list.append(key)\n",
    "            \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcy(partition, support, entire_size):\n",
    "    '''\n",
    "    perform pariori algorithm in the partition\n",
    "    with scaled down support threshold\n",
    "    '''\n",
    "    ## partition to baskets\n",
    "    baskets = merge_basket_2d(partition)\n",
    "    ## get ps\n",
    "    ps = scaled_down_support_threshold(baskets, support, entire_size)\n",
    "    \n",
    "    total_candidate = []\n",
    "    \n",
    "    ## singleton\n",
    "    freq_singleton, bitmap = singleton_count_bitmap(baskets, ps)\n",
    "    total_candidate += freq_singleton\n",
    "    \n",
    "    ## pairs\n",
    "    pair_size = 2\n",
    "    pair_list = combinations(freq_singleton, 2)\n",
    "    pair_count_dict = pair_counter(baskets, ps, freq_singleton, bitmap)\n",
    "    freq_singleton = filter_dict(pair_count_dict, ps)\n",
    "    candidate_list = freq_singleton\n",
    "    total_candidate += candidate_list\n",
    "    \n",
    "    ## above pair_size 3\n",
    "    while len(candidate_list)>0:\n",
    "        pair_size += 1 ## pair_size = 3\n",
    "        pair_list = generate_permutations(candidate_list, pair_size)\n",
    "        \n",
    "            \n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    return [candidate_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"small1.csv\"\n",
    "n = 15\n",
    "case = 1\n",
    "support = 4\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    textRDD = sc.textFile(input_file, n)\n",
    "    header = textRDD.first()  ## remove header\n",
    "    tmp = textRDD.filter(lambda x: x != header)\n",
    "    \n",
    "    \n",
    "    if case == 1:\n",
    "        baskets = tmp.map(lambda x: x.split(\",\")).mapValues(lambda x: [x]).reduceByKey(lambda a, b: a + b)\\\n",
    "        .mapValues(lambda x: sorted(list(set(x)))).map(lambda x: x[1])\n",
    "        basket_list = baskets.collect()\n",
    "        \n",
    "    elif case == 2 :\n",
    "        baskets = tmp.map(lambda x: [x.split(\",\")[1],x.split(\",\")[0]]).mapValues(lambda x: [x]).reduceByKey(lambda a, b: a + b)\\\n",
    "        .mapValues(lambda x: sorted(list(set(x)))).map(lambda x: x[1])\n",
    "        basket_list = baskets.collect()\n",
    "        \n",
    "    entire_size = len(basket_list)\n",
    "        \n",
    "    num = baskets.mapPartitions(lambda x: [sum(1 for i in x)]).collect() ## number of items in each partition\n",
    "       \n",
    "    ## Phase 1\n",
    "    \n",
    "    intermediate = baskets.mapPartitions(lambda x : pcy(x, support, entire_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(intermediate.collect()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
