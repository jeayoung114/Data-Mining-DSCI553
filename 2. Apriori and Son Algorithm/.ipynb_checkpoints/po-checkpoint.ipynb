{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW2 task1\n",
    "# Po-Han Chen USC ID:\n",
    "​\n",
    "from pyspark import SparkContext\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "​\n",
    "import functools as fc\n",
    "import itertools as it\n",
    "​\n",
    "# configuration on local machine\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/local/bin/python3.6'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/local/bin/python3.6'\n",
    "# parse command line arguments\n",
    "case_num = sys.argv[1]\n",
    "support = sys.argv[2]\n",
    "input_path = sys.argv[3]\n",
    "output_path = sys.argv[4]\n",
    "print(\"case=========\"+case_num)\n",
    "print(\"support=========\"+support)\n",
    "print(\"input=========\"+input_path)\n",
    "\"\"\"\n",
    "Define useful functions to help Spark implementation\n",
    "\"\"\"\n",
    "​\n",
    "​\n",
    "# finding candidates\n",
    "def add_size(iterator):\n",
    "    iter_list = list(iterator)\n",
    "    return [(ii[1], len(iter_list)) for ii in iter_list]\n",
    "​\n",
    "​\n",
    "def get_kvp_c(x):\n",
    "    return list(zip(list(x[0]), [1/x[1]] * len(x[0])))\n",
    "​\n",
    "​\n",
    "def get_dict(x, y):\n",
    "    x[y[0]] = x.get(y[0], 0) + y[1]\n",
    "    return x\n",
    "​\n",
    "​\n",
    "def count_par(iterator):\n",
    "    return [fc.reduce(get_dict, iterator, {})]\n",
    "​\n",
    "​\n",
    "# verifying candidates\n",
    "def get_kvp_v(x):\n",
    "    return list(zip(list(x[1]), [1] * len(x[1])))\n",
    "​\n",
    "​\n",
    "def printf(iterator):\n",
    "    print(list(iterator))\n",
    "    print(\"=\"*20)\n",
    "​\n",
    "​\n",
    "# getting new basket\n",
    "def flat_set(x):\n",
    "    if pass_num == 2:\n",
    "        return x\n",
    "    else:\n",
    "        new_basket = set()\n",
    "        print(x)\n",
    "        for xx in x[1]:\n",
    "            print(xx)\n",
    "        return x[0], new_basket\n",
    "​\n",
    "​\n",
    "\"\"\"\n",
    "Start timer\n",
    "\"\"\"\n",
    "start_time = time.time()\n",
    "sc = SparkContext('local[*]', 'task1')\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "# drop the header\n",
    "with open(input_path) as in_file:\n",
    "    data = in_file.read().splitlines(True)[1:]\n",
    "RDD = sc.parallelize(data)\n",
    "# keep track of pass\n",
    "pass_num = 1\n",
    "# initialize res string to write to file\n",
    "res = ''\n",
    "res_can = ''\n",
    "res_freq = ''\n",
    "# case 1\n",
    "if case_num == \"1\":\n",
    "    # map to key-value pairs\n",
    "    RDD = RDD.map(lambda x: tuple(x.strip().split(','))).partitionBy(1, lambda x: ord(x[0][0]))\n",
    "    # get baskets: user_id:[business_id, ...]\n",
    "    original_RDD = RDD.reduceByKey(lambda x, y: x+','+y).map(lambda x: (x[0], set(x[1].split(',')))).cache()\n",
    "    support_ratio = float(support)/RDD.count()\n",
    "# case 2\n",
    "elif case_num == \"2\":\n",
    "    # map to key-value pairs\n",
    "    RDD = RDD.map(lambda x: tuple(reversed(x.strip().split(',')))).partitionBy(1, lambda x: ord(x[0][0]))\n",
    "    # get baskets: user_id:[business_id, ...]\n",
    "    original_RDD = RDD.reduceByKey(lambda x, y: x + ',' + y).map(lambda x: (x[0], set(x[1].split(',')))).cache()\n",
    "    support_ratio = float(support) / RDD.count()\n",
    "# a-priori\n",
    "while True:\n",
    "    if pass_num == 1:\n",
    "        basket_RDD = original_RDD\n",
    "    # starting from pass 2, create new basket with size = pass_num itemsets\n",
    "    elif pass_num == 2:\n",
    "        basket_RDD = original_RDD.map(lambda x: (x[0], x[1].intersection(freq_sets_flat))).map(\n",
    "            lambda x: (x[0], {tuple(sorted(xx)) for xx in set(it.combinations(x[1], pass_num))}) if len(\n",
    "                x[1]) >= pass_num else (x[0], set()))\n",
    "    elif pass_num == 3:\n",
    "        # delete non frequent items from basket and create new baskets\n",
    "        basket_RDD = original_RDD.map(lambda x: (x[0], x[1].intersection(freq_sets_flat))).map(\n",
    "            lambda x: (x[0], {tuple(sorted(xx)) for xx in set(it.combinations(x[1], pass_num-1))}) if len(\n",
    "                x[1]) >= pass_num-1 else (x[0], set())).map(lambda x: (x[0], x[1].intersection(freq_sets))).map(\n",
    "            lambda x: (x[0], set([xxx for xx in x[1] for xxx in xx])) if x[1] != set() else (x[0], set())).map(\n",
    "            lambda x: (x[0], {tuple(sorted(xx)) for xx in set(it.combinations(x[1], pass_num))}) if len(\n",
    "                x[1]) >= pass_num else (x[0], set()))\n",
    "    # get candidates\n",
    "​\n",
    "    candidates = basket_RDD.mapPartitions(add_size).filter(lambda x: x[0] != set()).flatMap(\n",
    "        get_kvp_c).mapPartitions(count_par).flatMap(\n",
    "        lambda x: [k for k, v in x.items() if v >= support_ratio]).collect()\n",
    "    \"\"\"\n",
    "    Needs rework on sorting the tuples (keys)\n",
    "    \"\"\"\n",
    "    candidates = sorted(list(set(candidates)))\n",
    "    # if candidates is empty: break loop\n",
    "    if not candidates:\n",
    "        break\n",
    "    for can in candidates:\n",
    "        if pass_num == 1:\n",
    "            res_can += \"('\"+can+\"'),\"\n",
    "        else:\n",
    "            res_can += str(can)+\",\"\n",
    "    res_can = res_can.rstrip(',') + '\\n\\n'\n",
    "    candidates = set([can for can in candidates])\n",
    "    # verify candidates\n",
    "    freq_sets = basket_RDD.map(lambda x: (x[0], x[1].intersection(candidates))).flatMap(\n",
    "        get_kvp_v).reduceByKey(lambda x, y: x+y).filter(lambda x: x[1] >= int(support)).collect()\n",
    "    \"\"\"\n",
    "    Needs rework on sorting the tuples (keys)\n",
    "    \"\"\"\n",
    "    freq_sets = sorted(freq_sets)\n",
    "    # if freq_sets is empty: break loop\n",
    "    if not freq_sets:\n",
    "        break\n",
    "    for freq in freq_sets:\n",
    "        if pass_num == 1:\n",
    "            res_freq += \"('\"+freq[0]+\"'),\"\n",
    "        else:\n",
    "            res_freq += str(tuple(sorted(freq[0]))) + \",\"\n",
    "    res_freq = res_freq.rstrip(',') + '\\n\\n'\n",
    "    if pass_num == 1:\n",
    "        freq_sets_flat = set([freq[0] for freq in freq_sets])\n",
    "    else:\n",
    "        freq_sets_flat = set([ff for freq in freq_sets for ff in freq[0]])\n",
    "    freq_sets = set([freq[0] for freq in freq_sets])\n",
    "    pass_num += 1\n",
    "# write file\n",
    "res += \"Candidates:\\n\" + res_can + \"Frequent Itemsets:\\n\" + res_freq.rstrip('\\n')\n",
    "with open(output_path, 'w') as out_file:\n",
    "    out_file.writelines(res)\n",
    "\"\"\"\n",
    "End timer\n",
    "\"\"\"\n",
    "duration = time.time() - start_time\n",
    "print(\"Duration: \"+str(duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
